<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>VoiceVision - Gemma 3n AI Assistant</title>

    <!-- Add Tesseract.js for real OCR -->
    <script src="https://unpkg.com/tesseract.js@5.0.4/dist/tesseract.min.js"></script>
    <!-- Add TensorFlow.js for object detection -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.15.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>
    <!-- Add MediaPipe for sign language detection -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    
    <!-- Environment Configuration -->
    <script src="vercel-env.js" onerror="console.log('Vercel env not found - using fallback')"></script>
    <script src="env-loader.js"></script>
    <script src="config.js"></script>
    
    <!-- Add Google Generative AI for Gemma 3n -->
    <script type="importmap">
      {
        "imports": {
          "@google/generative-ai": "https://esm.run/@google/generative-ai"
        }
      }
    </script>
    <style>
      :root {
        --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        --card-gradient: linear-gradient(145deg, #ffffff 0%, #f8f9fa 100%);
        --shadow-light: 0 8px 32px rgba(0, 0, 0, 0.1);
        --shadow-medium: 0 12px 40px rgba(0, 0, 0, 0.15);
        --shadow-heavy: 0 20px 60px rgba(0, 0, 0, 0.2);
        --border-radius: 16px;
        --border-radius-small: 12px;
        --transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
        --text-primary: #2d3748;
        --text-secondary: #4a5568;
        --accent-blue: #3182ce;
        --accent-green: #38a169;
        --accent-orange: #ed8936;
        --accent-red: #e53e3e;
        --accent-purple: #805ad5;
      }

      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
        color: var(--text-primary);
        background: var(--primary-gradient);
        min-height: 100vh;
        overflow-x: hidden;
      }

      .container {
        max-width: 1400px;
        margin: 0 auto;
        padding: 20px;
      }

      .header {
        text-align: center;
        color: white;
        margin-bottom: 40px;
        padding: 40px 20px;
        backdrop-filter: blur(10px);
        background: rgba(255, 255, 255, 0.1);
        border-radius: var(--border-radius);
        border: 1px solid rgba(255, 255, 255, 0.2);
      }

      .header h1 {
        font-size: clamp(2.5rem, 5vw, 4rem);
        margin-bottom: 16px;
        text-shadow: 2px 2px 20px rgba(0, 0, 0, 0.3);
        font-weight: 700;
        background: linear-gradient(45deg, #fff, #e2e8f0);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }

      .header p {
        font-size: clamp(1.1rem, 2.5vw, 1.4rem);
        opacity: 0.95;
        max-width: 600px;
        margin: 0 auto;
        font-weight: 400;
      }

      .main-content {
        background: var(--card-gradient);
        border-radius: var(--border-radius);
        padding: 40px;
        box-shadow: var(--shadow-heavy);
        margin-bottom: 30px;
        backdrop-filter: blur(20px);
        border: 1px solid rgba(255, 255, 255, 0.2);
        position: relative;
        overflow: hidden;
      }

      .main-content::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 4px;
        background: linear-gradient(
          90deg,
          var(--accent-blue),
          var(--accent-purple),
          var(--accent-green)
        );
      }

      .camera-section {
        text-align: center;
        margin-bottom: 40px;
        position: relative;
        display: flex;
        justify-content: center;
        align-items: center;
        width: 100%;
      }

      #camera-feed {
        max-width: 100%;
        width: 100%;
        max-width: 720px;
        height: auto;
        aspect-ratio: 16/9;
        border-radius: var(--border-radius);
        box-shadow: var(--shadow-medium);
        background: linear-gradient(145deg, #f7fafc, #edf2f7);
        border: 2px solid rgba(255, 255, 255, 0.8);
        object-fit: cover;
        transition: var(--transition);
      }

      #camera-feed:hover {
        transform: scale(1.02);
        box-shadow: var(--shadow-heavy);
      }

      #hand-landmarks {
        position: absolute;
        top: 2px;
        left: 50%;
        transform: translateX(-50%);
        border-radius: var(--border-radius);
        pointer-events: none;
        z-index: 10;
        opacity: 0;
        display: none;
      }

      .controls {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 20px;
        margin: 40px 0;
      }

      .btn {
        padding: 18px 32px;
        font-size: 16px;
        font-weight: 600;
        border: none;
        border-radius: var(--border-radius-small);
        cursor: pointer;
        transition: var(--transition);
        text-transform: none;
        letter-spacing: 0.5px;
        position: relative;
        overflow: hidden;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.2);
      }

      .btn::before {
        content: "";
        position: absolute;
        top: 0;
        left: -100%;
        width: 100%;
        height: 100%;
        background: linear-gradient(
          90deg,
          transparent,
          rgba(255, 255, 255, 0.2),
          transparent
        );
        transition: left 0.6s;
      }

      .btn:hover::before {
        left: 100%;
      }

      .btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none !important;
      }

      .btn:disabled::before {
        display: none;
      }

      .btn-primary {
        background: linear-gradient(135deg, var(--accent-blue), #2c5aa0);
        color: white;
        box-shadow: 0 8px 25px rgba(49, 130, 206, 0.3);
      }

      .btn-primary:hover:not(:disabled) {
        transform: translateY(-3px) scale(1.02);
        box-shadow: 0 12px 35px rgba(49, 130, 206, 0.4);
      }

      .btn-success {
        background: linear-gradient(135deg, var(--accent-green), #2f855a);
        color: white;
        box-shadow: 0 8px 25px rgba(56, 161, 105, 0.3);
      }

      .btn-success:hover:not(:disabled) {
        transform: translateY(-3px) scale(1.02);
        box-shadow: 0 12px 35px rgba(56, 161, 105, 0.4);
      }

      .btn-info {
        background: linear-gradient(135deg, #3182ce, #2c5aa0);
        color: white;
        box-shadow: 0 8px 25px rgba(49, 130, 206, 0.3);
      }

      .btn-info:hover:not(:disabled) {
        transform: translateY(-3px) scale(1.02);
        box-shadow: 0 12px 35px rgba(49, 130, 206, 0.4);
      }

      .btn-warning {
        background: linear-gradient(135deg, var(--accent-orange), #c05621);
        color: white;
        box-shadow: 0 8px 25px rgba(237, 137, 54, 0.3);
      }

      .btn-warning:hover:not(:disabled) {
        transform: translateY(-3px) scale(1.02);
        box-shadow: 0 12px 35px rgba(237, 137, 54, 0.4);
      }

      .btn-danger {
        background: linear-gradient(135deg, var(--accent-red), #c53030);
        color: white;
        box-shadow: 0 8px 25px rgba(229, 62, 62, 0.3);
      }

      .btn-danger:hover:not(:disabled) {
        transform: translateY(-3px) scale(1.02);
        box-shadow: 0 12px 35px rgba(229, 62, 62, 0.4);
      }

      .btn-secondary {
        background: linear-gradient(135deg, #718096, #4a5568);
        color: white;
        box-shadow: 0 8px 25px rgba(113, 128, 150, 0.3);
      }

      .btn-secondary:hover:not(:disabled) {
        transform: translateY(-3px) scale(1.02);
        box-shadow: 0 12px 35px rgba(113, 128, 150, 0.4);
      }

      .status-panel {
        background: linear-gradient(145deg, #f7fafc, #edf2f7);
        padding: 30px;
        border-radius: var(--border-radius);
        border-left: 6px solid var(--accent-blue);
        margin: 30px 0;
        min-height: 140px;
        box-shadow: var(--shadow-light);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.3);
      }

      .status-panel h3 {
        color: var(--accent-blue);
        margin-bottom: 16px;
        font-size: 1.4rem;
        font-weight: 700;
      }

      .status-text {
        font-size: 17px;
        line-height: 1.6;
        color: var(--text-secondary);
      }

      .loading {
        display: inline-block;
        width: 24px;
        height: 24px;
        border: 3px solid rgba(49, 130, 206, 0.2);
        border-radius: 50%;
        border-top-color: var(--accent-blue);
        animation: spin 1s ease-in-out infinite;
        margin-right: 12px;
      }

      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }

      .keyboard-shortcuts {
        background: linear-gradient(145deg, #ffffff, #f7fafc);
        padding: 30px;
        border-radius: var(--border-radius);
        margin: 30px 0;
        box-shadow: var(--shadow-light);
        border: 1px solid rgba(255, 255, 255, 0.3);
      }

      .keyboard-shortcuts h4 {
        color: var(--text-primary);
        margin-bottom: 20px;
        font-size: 1.3rem;
        font-weight: 700;
        text-align: center;
      }

      .shortcut {
        display: inline-block;
        background: linear-gradient(145deg, var(--accent-blue), #2c5aa0);
        color: white;
        padding: 8px 16px;
        border-radius: 8px;
        font-weight: 600;
        font-size: 14px;
        margin: 8px 12px 8px 0;
        box-shadow: 0 4px 12px rgba(49, 130, 206, 0.3);
        transition: var(--transition);
      }

      .shortcut:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(49, 130, 206, 0.4);
      }

      .features {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
        gap: 30px;
        margin-top: 40px;
      }

      .feature-card {
        background: linear-gradient(145deg, #ffffff, #f7fafc);
        padding: 32px;
        border-radius: var(--border-radius);
        box-shadow: var(--shadow-light);
        transition: var(--transition);
        border: 1px solid rgba(255, 255, 255, 0.3);
        position: relative;
        overflow: hidden;
      }

      .feature-card::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 4px;
        background: linear-gradient(
          90deg,
          var(--accent-blue),
          var(--accent-purple)
        );
        opacity: 0;
        transition: var(--transition);
      }

      .feature-card:hover {
        transform: translateY(-8px);
        box-shadow: var(--shadow-medium);
      }

      .feature-card:hover::before {
        opacity: 1;
      }

      .feature-card h4 {
        color: var(--text-primary);
        margin-bottom: 16px;
        font-size: 1.3rem;
        font-weight: 700;
      }

      .feature-card p {
        color: var(--text-secondary);
        line-height: 1.7;
        font-size: 15px;
      }

      /* Responsive Design */
      @media (max-width: 768px) {
        .container {
          padding: 15px;
        }

        .header {
          padding: 30px 15px;
          margin-bottom: 30px;
        }

        .main-content {
          padding: 25px;
          margin-bottom: 20px;
        }

        .controls {
          grid-template-columns: 1fr;
          gap: 15px;
        }

        .btn {
          padding: 16px 24px;
          font-size: 15px;
        }

        .features {
          grid-template-columns: 1fr;
          gap: 20px;
        }

        .keyboard-shortcuts {
          padding: 20px;
        }

        .shortcut {
          padding: 6px 12px;
          font-size: 13px;
          margin: 6px 8px 6px 0;
        }
      }

      @media (max-width: 480px) {
        .header h1 {
          font-size: 2rem;
        }

        .header p {
          font-size: 1rem;
        }

        .main-content {
          padding: 20px;
        }

        .status-panel {
          padding: 20px;
          min-height: 100px;
        }

        .feature-card {
          padding: 24px;
        }
      }

      /* Dark mode support for accessibility */
      @media (prefers-color-scheme: dark) {
        .feature-card,
        .keyboard-shortcuts,
        .status-panel {
          background: linear-gradient(145deg, #2d3748, #1a202c);
          color: #e2e8f0;
        }

        .feature-card h4,
        .keyboard-shortcuts h4 {
          color: #e2e8f0;
        }

        .feature-card p {
          color: #cbd5e0;
        }

        .status-text {
          color: #cbd5e0;
        }
      }

      /* Focus styles for accessibility */
      .btn:focus,
      .shortcut:focus {
        outline: 3px solid rgba(49, 130, 206, 0.5);
        outline-offset: 2px;
      }

      /* Animation for page load */
      @keyframes fadeInUp {
        from {
          opacity: 0;
          transform: translateY(30px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .main-content,
      .keyboard-shortcuts,
      .feature-card {
        animation: fadeInUp 0.6s ease-out;
      }

      .feature-card:nth-child(2) {
        animation-delay: 0.1s;
      }

      .feature-card:nth-child(3) {
        animation-delay: 0.2s;
      }

      .feature-card:nth-child(4) {
        animation-delay: 0.3s;
      }

      .feature-card:nth-child(5) {
        animation-delay: 0.4s;
      }

      .feature-card:nth-child(6) {
        animation-delay: 0.5s;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="header">
        <h1>🎯 VoiceVision</h1>
        <p>
          Powered by Gemma 3n Multimodal AI - Real-Time Assistant for Visually
          Impaired
        </p>
      </div>

      <div class="accessibility-notice">
        <h4>♿ Accessibility Features</h4>
        <p>
          This application provides voice-guided navigation, real-time scene
          description, text reading, and object identification. All features are
          keyboard accessible and include audio feedback.
        </p>
      </div>

      <div class="main-content">
        <div class="camera-section">
          <video id="camera-feed" autoplay playsinline muted></video>
          <canvas
            id="hand-landmarks"
            style="
              position: absolute;
              top: 0;
              left: 0;
              pointer-events: none;
              z-index: 10;
            "
          ></canvas>
        </div>

        <div class="controls">
          <button id="start-btn" class="btn btn-primary">
            📷 Start Camera & AI
          </button>
          <button id="describe-btn" class="btn btn-success" disabled>
            🔍 Describe Scene
          </button>
          <button id="read-btn" class="btn btn-info" disabled>
            📖 Read Text (OCR)
          </button>
          <button id="objects-btn" class="btn btn-warning" disabled>
            🎯 Identify Objects
          </button>
          <button id="stop-btn" class="btn btn-danger" disabled>
            🛑 Stop Camera
          </button>
          <button id="stop-speech-btn" class="btn btn-secondary">
            🔇 Stop Speech
          </button>
          <button id="voice-select-btn" class="btn btn-secondary">
            🎤 Voice Settings
          </button>
          <button id="language-btn" class="btn btn-secondary">
            🌍 Language
          </button>
          <button id="sign-language-btn" class="btn btn-secondary">
            🤟 Sign Language: OFF
          </button>
          <button id="api-key-btn" class="btn btn-secondary">
            ℹ️ API Info
          </button>
        </div>

        <div class="status-panel">
          <h3>📢 System Status</h3>
          <div id="status-text" class="status-text">
            Ready to start. Click "Start Camera & AI" to begin VoiceVision
            assistance.
          </div>
        </div>

        <div class="keyboard-shortcuts">
          <h4>⌨️ Keyboard Shortcuts & Sign Language</h4>
          <span class="shortcut">D</span> Describe Scene
          <span class="shortcut">R</span> Read Text
          <span class="shortcut">O</span> Identify Objects
          <span class="shortcut">Q</span> Stop Camera
          <span class="shortcut">S</span> Stop Speech
          <span class="shortcut">Space</span> Repeat Last Description
          <br /><br />
          <strong>🤟 Sign Language Gestures:</strong><br />
          <span class="shortcut">👆</span> Index finger - Describe Scene<br />
          <span class="shortcut">✌️</span> Two fingers - Read Text<br />
          <span class="shortcut">🖗</span> Three fingers - Navigation Help<br />
          <span class="shortcut">🤙</span> Pinky finger - Identify Objects<br />
          <span class="shortcut">💍</span> Ring finger - Voice Settings<br />
          <span class="shortcut">🖕</span> Middle finger - Language Settings<br />
          <span class="shortcut">🤏</span> Thumb + Index - Brightness<br />
          <span class="shortcut">🤌</span> Thumb + Middle - Emergency Mode<br />
          <span class="shortcut">📸</span> Thumb + Index + Middle - Take
          Photo<br />
          <span class="shortcut">🔊</span> 4 fingers - Volume Control<br />
          <span class="shortcut">✋</span> All fingers - Stop Camera<br />
          <span class="shortcut">👍</span> Thumb only - Repeat Last<br />
          <span class="shortcut">✊</span> Closed fist - Pause/Resume<br />
          <span class="shortcut">🤘</span> Index + Ring - Help Tutorial<br />
          <span class="shortcut">🤙</span> Thumb + Pinky - Speech Speed
          <br /><br />
          <strong>🎤 Voice Commands for Settings:</strong><br />
          <span class="shortcut">Voice Settings</span> "faster", "slower", "louder", "quieter", "next voice", "previous voice", "test voice"<br />
          <span class="shortcut">Language Settings</span> "next language", "previous language", or say language name like "Spanish"<br />
          <strong>🤟 Settings Gestures:</strong><br />
          <span class="shortcut">👍</span> Thumbs up - Faster speech<br />
          <span class="shortcut">👎</span> Thumbs down - Slower speech<br />
          <span class="shortcut">👈</span> Point left - Previous voice/language<br />
          <span class="shortcut">👉</span> Point right - Next voice<br />
          <span class="shortcut">☝️</span> Point up - Next language<br />
          <span class="shortcut">👇</span> Point down - Previous language<br />
          <span class="shortcut">✌️</span> Peace sign - Test voice/Select language<br />
          <span class="shortcut">✊</span> Closed fist - Exit settings</span>
        </div>

        <div class="features">
          <div class="feature-card">
            <h4>🧠 Gemma 3n Multimodal AI</h4>
            <p>
              Advanced AI provides intelligent scene understanding, natural
              language descriptions, and contextual awareness beyond basic
              object detection.
            </p>
          </div>
          <div class="feature-card">
            <h4>🔍 Real-Time Scene Analysis</h4>
            <p>
              Combines TensorFlow.js object detection with Gemma 3n's
              intelligence to provide detailed, conversational scene
              descriptions with spatial awareness.
            </p>
          </div>
          <div class="feature-card">
            <h4>📖 Optical Character Recognition</h4>
            <p>
              Tesseract.js OCR engine reads text from signs, documents, menus,
              and labels with high accuracy, even in challenging lighting
              conditions.
            </p>
          </div>
          <div class="feature-card">
            <h4>🎯 Intelligent Object Detection</h4>
            <p>
              Gemma 3n enhances object recognition with contextual
              understanding, providing detailed descriptions of objects, their
              purpose, and interaction possibilities.
            </p>
          </div>
          <div class="feature-card">
            <h4>🔊 Voice Feedback</h4>
            <p>
              All analysis results are provided through clear voice synthesis,
              allowing hands-free operation and seamless accessibility.
            </p>
          </div>
          <div class="feature-card">
            <h4>🤟 Sign Language Support</h4>
            <p>
              Real-time hand tracking and gesture recognition enables deaf-blind
              users to interact through sign language gestures for common
              commands like "describe scene", "read text", and "identify
              objects".
            </p>
          </div>
        </div>
      </div>
    </div>

    <script type="module">
      // Import Google Generative AI for Gemma 3n
      import { GoogleGenerativeAI } from "@google/generative-ai";

      // VoiceVision with Gemma 3n Integration
      class VoiceVisionInterface {
        constructor() {
          this.video = document.getElementById("camera-feed");
          this.statusText = document.getElementById("status-text");
          this.stream = null;
          this.isRunning = false;
          this.lastDescription = "";

          // AI Models
          this.cocoModel = null; // TensorFlow COCO-SSD for fast object detection
          this.tesseractWorker = null; // Tesseract for OCR
          this.gemmaModel = null; // Gemma 3n for intelligent scene understanding
          this.genAI = null; // Google Generative AI client

          // Voice Settings
          this.availableVoices = [];
          this.selectedVoice = null;
          this.voiceSettings = {
            rate: 0.9,
            pitch: 1.0,
            volume: 0.8,
          };

          // Sign Language Settings
          this.signLanguageEnabled =
            localStorage.getItem("voicevision_sign_language") === "true" ||
            false;
          this.handsModel = null;
          this.handLandmarksCanvas = document.getElementById("hand-landmarks");
          this.handLandmarksCtx = this.handLandmarksCanvas.getContext("2d");
          this.lastGestureTime = 0;
          this.gestureBuffer = [];
          this.gestureThreshold = 30; // frames to confirm gesture

          // System State
          this.emergencyMode = false;
          this.systemPaused = false;
          this.savedPhoto = null;
          
          // Settings Mode States
          this.voiceSelectionMode = false;
          this.languageSelectionMode = false;
          this.voiceSelectionTimeout = null;
          this.languageSelectionTimeout = null;
          this.currentLanguageIndex = 0;
          this.languageOptions = [];
          this.currentVoiceIndex = 0;
          
          // Voice Recognition
          this.recognition = null;
          this.isProcessing = false;

          // Language Settings
          this.currentLanguage =
            localStorage.getItem("voicevision_language") || "en";
          this.languages = {
            en: "English",
            es: "Español",
            fr: "Français",
            de: "Deutsch",
            it: "Italiano",
            pt: "Português",
            ru: "Русский",
            zh: "中文",
            ja: "日本語",
            ko: "한국어",
            ar: "العربية",
            hi: "हिन्दी",
          };

          // Language-specific prompts for Gemma 3n AI
          this.languagePrompts = {
            en: {
              sceneAnalysis: `You are VoiceVision, an AI assistant for visually impaired users. Analyze this image and provide a clear, helpful description of what you see. Focus on:

1. Main objects and people present
2. Their locations and spatial relationships  
3. The environment/setting (indoor/outdoor, lighting, etc.)
4. Any text or signs visible
5. Potential hazards or navigation information

Provide a natural, conversational description as if speaking directly to a visually impaired person. Be specific but concise. Start with "I can see..." `,
              objectIdentification: `You are VoiceVision, helping a visually impaired person identify objects. Analyze this image and:

1. List all objects you can clearly identify with their approximate locations (left, center, right, top, bottom)
2. Describe their size, color, and condition if relevant
3. Mention any useful details for a visually impaired person (buttons, handles, text on objects, etc.)
4. If you see people, describe their general position and activities
5. Focus on actionable information

Provide a clear, organized response that helps the person understand what's in their environment. Start with "I can identify..." `,
            },
            es: {
              sceneAnalysis: `Eres VoiceVision, un asistente de IA para usuarios con discapacidad visual. Analiza esta imagen y proporciona una descripción clara y útil de lo que ves. Enfócate en:

1. Objetos principales y personas presentes
2. Sus ubicaciones y relaciones espaciales
3. El entorno/ambiente (interior/exterior, iluminación, etc.)
4. Cualquier texto o señal visible
5. Peligros potenciales o información de navegación

Proporciona una descripción natural y conversacional como si hablaras directamente con una persona con discapacidad visual. Sé específico pero conciso. Comienza con "Puedo ver..." `,
              objectIdentification: `Eres VoiceVision, ayudando a una persona con discapacidad visual a identificar objetos. Analiza esta imagen y:

1. Lista todos los objetos que puedes identificar claramente con sus ubicaciones aproximadas (izquierda, centro, derecha, arriba, abajo)
2. Describe su tamaño, color y condición si es relevante
3. Menciona cualquier detalle útil para una persona con discapacidad visual (botones, manijas, texto en objetos, etc.)
4. Si ves personas, describe su posición general y actividades
5. Enfócate en información útil

Proporciona una respuesta clara y organizada que ayude a la persona a entender qué hay en su entorno. Comienza con "Puedo identificar..." `,
            },
            fr: {
              sceneAnalysis: `Vous êtes VoiceVision, un assistant IA pour les utilisateurs malvoyants. Analysez cette image et fournissez une description claire et utile de ce que vous voyez. Concentrez-vous sur :

1. Les objets principaux et les personnes présentes
2. Leurs emplacements et relations spatiales
3. L'environnement/cadre (intérieur/extérieur, éclairage, etc.)
4. Tout texte ou panneau visible
5. Les dangers potentiels ou informations de navigation

Fournissez une description naturelle et conversationnelle comme si vous parliez directement à une personne malvoyante. Soyez précis mais concis. Commencez par "Je peux voir..." `,
              objectIdentification: `Vous êtes VoiceVision, aidant une personne malvoyante à identifier des objets. Analysez cette image et :

1. Listez tous les objets que vous pouvez identifier clairement avec leurs emplacements approximatifs (gauche, centre, droite, haut, bas)
2. Décrivez leur taille, couleur et condition si pertinent
3. Mentionnez tout détail utile pour une personne malvoyante (boutons, poignées, texte sur objets, etc.)
4. Si vous voyez des personnes, décrivez leur position générale et activités
5. Concentrez-vous sur l'information exploitable

Fournissez une réponse claire et organisée qui aide la personne à comprendre ce qui est dans son environnement. Commencez par "Je peux identifier..." `,
            },
            de: {
              sceneAnalysis: `Sie sind VoiceVision, ein KI-Assistent für sehbehinderte Benutzer. Analysieren Sie dieses Bild und geben Sie eine klare, hilfreiche Beschreibung dessen, was Sie sehen. Konzentrieren Sie sich auf:

1. Hauptobjekte und anwesende Personen
2. Ihre Standorte und räumlichen Beziehungen
3. Die Umgebung/das Setting (innen/außen, Beleuchtung, etc.)
4. Sichtbare Texte oder Schilder
5. Potenzielle Gefahren oder Navigationsinformationen

Geben Sie eine natürliche, gesprächige Beschreibung, als würden Sie direkt mit einer sehbehinderten Person sprechen. Seien Sie spezifisch aber prägnant. Beginnen Sie mit "Ich kann sehen..." `,
              objectIdentification: `Sie sind VoiceVision und helfen einer sehbehinderten Person bei der Objektidentifikation. Analysieren Sie dieses Bild und:

1. Listen Sie alle Objekte auf, die Sie klar identifizieren können, mit ihren ungefähren Positionen (links, Mitte, rechts, oben, unten)
2. Beschreiben Sie ihre Größe, Farbe und Zustand, falls relevant
3. Erwähnen Sie nützliche Details für eine sehbehinderte Person (Knöpfe, Griffe, Text auf Objekten, etc.)
4. Wenn Sie Personen sehen, beschreiben Sie ihre allgemeine Position und Aktivitäten
5. Konzentrieren Sie sich auf umsetzbare Informationen

Geben Sie eine klare, organisierte Antwort, die der Person hilft, zu verstehen, was in ihrer Umgebung ist. Beginnen Sie mit "Ich kann identifizieren..." `,
            },
            it: {
              sceneAnalysis: `Sei VoiceVision, un assistente IA per utenti ipovedenti. Analizza questa immagine e fornisci una descrizione chiara e utile di ciò che vedi. Concentrati su:

1. Oggetti principali e persone presenti
2. Le loro posizioni e relazioni spaziali
3. L'ambiente/contesto (interno/esterno, illuminazione, ecc.)
4. Qualsiasi testo o cartello visibile
5. Pericoli potenziali o informazioni di navigazione

Fornisci una descrizione naturale e conversazionale come se stessi parlando direttamente con una persona ipovedente. Sii specifico ma conciso. Inizia con "Posso vedere..." `,
              objectIdentification: `Sei VoiceVision, aiuti una persona ipovedente a identificare oggetti. Analizza questa immagine e:

1. Elenca tutti gli oggetti che puoi identificare chiaramente con le loro posizioni approssimative (sinistra, centro, destra, alto, basso)
2. Descrivi le loro dimensioni, colore e condizioni se rilevanti
3. Menziona qualsiasi dettaglio utile per una persona ipovedente (pulsanti, maniglie, testo sugli oggetti, ecc.)
4. Se vedi persone, descrivi la loro posizione generale e attività
5. Concentrati su informazioni utilizzabili

Fornisci una risposta chiara e organizzata che aiuti la persona a capire cosa c'è nel loro ambiente. Inizia con "Posso identificare..." `,
            },
            pt: {
              sceneAnalysis: `Você é VoiceVision, um assistente de IA para usuários com deficiência visual. Analise esta imagem e forneça uma descrição clara e útil do que você vê. Foque em:

1. Objetos principais e pessoas presentes
2. Suas localizações e relações espaciais
3. O ambiente/configuração (interno/externo, iluminação, etc.)
4. Qualquer texto ou sinal visível
5. Perigos potenciais ou informações de navegação

Forneça uma descrição natural e conversacional como se estivesse falando diretamente com uma pessoa com deficiência visual. Seja específico mas conciso. Comece com "Posso ver..." `,
              objectIdentification: `Você é VoiceVision, ajudando uma pessoa com deficiência visual a identificar objetos. Analise esta imagem e:

1. Liste todos os objetos que você pode identificar claramente com suas localizações aproximadas (esquerda, centro, direita, topo, baixo)
2. Descreva seu tamanho, cor e condição se relevante
3. Mencione qualquer detalhe útil para uma pessoa com deficiência visual (botões, alças, texto em objetos, etc.)
4. Se você vir pessoas, descreva sua posição geral e atividades
5. Foque em informações acionáveis

Forneça uma resposta clara e organizada que ajude a pessoa a entender o que está em seu ambiente. Comece com "Posso identificar..." `,
            },
            ru: {
              sceneAnalysis: `Вы VoiceVision, ИИ-помощник для слабовидящих пользователей. Проанализируйте это изображение и предоставьте четкое, полезное описание того, что вы видите. Сосредоточьтесь на:

1. Основных объектах и присутствующих людях
2. Их расположении и пространственных отношениях
3. Окружении/обстановке (внутри/снаружи, освещение и т.д.)
4. Любом видимом тексте или знаках
5. Потенциальных опасностях или навигационной информации

Предоставьте естественное, разговорное описание, как если бы вы говорили напрямую со слабовидящим человеком. Будьте конкретны, но лаконичны. Начните с "Я вижу..." `,
              objectIdentification: `Вы VoiceVision, помогаете слабовидящему человеку идентифицировать объекты. Проанализируйте это изображение и:

1. Перечислите все объекты, которые вы можете четко идентифицировать с их приблизительными местоположениями (слева, в центре, справа, вверху, внизу)
2. Опишите их размер, цвет и состояние, если это уместно
3. Упомяните любые полезные детали для слабовидящего человека (кнопки, ручки, текст на объектах и т.д.)
4. Если вы видите людей, опишите их общее положение и деятельность
5. Сосредоточьтесь на практической информации

Предоставьте четкий, организованный ответ, который поможет человеку понять, что находится в его окружении. Начните с "Я могу идентифицировать..." `,
            },
            zh: {
              sceneAnalysis: `您是VoiceVision，为视障用户提供服务的AI助手。分析这张图片并提供您所看到内容的清晰、有用描述。重点关注：

1. 主要物体和在场人员
2. 它们的位置和空间关系
3. 环境/场景（室内/室外、照明等）
4. 任何可见的文字或标志
5. 潜在危险或导航信息

提供自然、对话式的描述，就像直接与视障人士交谈一样。要具体但简洁。以"我可以看到..."开始 `,
              objectIdentification: `您是VoiceVision，帮助视障人士识别物体。分析这张图片并：

1. 列出您能清楚识别的所有物体及其大致位置（左侧、中央、右侧、上方、下方）
2. 如相关，描述它们的大小、颜色和状态
3. 提及对视障人士有用的任何细节（按钮、把手、物体上的文字等）
4. 如果您看到人，描述他们的大致位置和活动
5. 专注于可操作的信息

提供清晰、有条理的回答，帮助这个人理解他们环境中的内容。以"我可以识别..."开始 `,
            },
            ja: {
              sceneAnalysis: `あなたはVoiceVisionです。視覚障害者のユーザーのためのAIアシスタントです。この画像を分析し、見えるものの明確で役立つ説明を提供してください。以下に焦点を当ててください：

1. 主要な物体と存在する人々
2. それらの位置と空間的関係
3. 環境/設定（屋内/屋外、照明など）
4. 見える文字や標識
5. 潜在的な危険やナビゲーション情報

視覚障害者と直接話すような自然で会話的な説明を提供してください。具体的だが簡潔にしてください。「私には...が見えます」で始めてください `,
              objectIdentification: `あなたはVoiceVisionです。視覚障害者の物体識別を支援しています。この画像を分析し：

1. 明確に識別できるすべての物体をおおよその位置とともに一覧表示してください（左、中央、右、上、下）
2. 関連があれば、サイズ、色、状態を説明してください
3. 視覚障害者にとって有用な詳細を言及してください（ボタン、ハンドル、物体上のテキストなど）
4. 人が見える場合、その一般的な位置と活動を説明してください
5. 実用的な情報に焦点を当ててください

その人が環境にあるものを理解するのに役立つ明確で整理された回答を提供してください。「私は...を識別できます」で始めてください `,
            },
            ko: {
              sceneAnalysis: `당신은 시각 장애인 사용자를 위한 AI 어시스턴트인 VoiceVision입니다. 이 이미지를 분석하고 보이는 것에 대한 명확하고 유용한 설명을 제공하세요. 다음에 집중하세요:

1. 주요 물체와 현재 있는 사람들
2. 그들의 위치와 공간적 관계
3. 환경/설정 (실내/실외, 조명 등)
4. 보이는 텍스트나 표지판
5. 잠재적 위험이나 내비게이션 정보

시각 장애인과 직접 대화하는 것처럼 자연스럽고 대화적인 설명을 제공하세요. 구체적이지만 간결하게 하세요. "저는 ...을 볼 수 있습니다"로 시작하세요 `,
              objectIdentification: `당신은 시각 장애인이 물체를 식별하도록 돕는 VoiceVision입니다. 이 이미지를 분석하고:

1. 명확하게 식별할 수 있는 모든 물체를 대략적인 위치와 함께 나열하세요 (왼쪽, 중앙, 오른쪽, 위, 아래)
2. 관련이 있다면 크기, 색상, 상태를 설명하세요
3. 시각 장애인에게 유용한 세부사항을 언급하세요 (버튼, 손잡이, 물체의 텍스트 등)
4. 사람이 보인다면 일반적인 위치와 활동을 설명하세요
5. 실행 가능한 정보에 집중하세요

그 사람이 환경에 무엇이 있는지 이해하는 데 도움이 되는 명확하고 체계적인 답변을 제공하세요. "저는 ...을 식별할 수 있습니다"로 시작하세요 `,
            },
            ar: {
              sceneAnalysis: `أنت VoiceVision، مساعد ذكي للمستخدمين ضعاف البصر. حلل هذه الصورة وقدم وصفاً واضحاً ومفيداً لما تراه. ركز على:

1. الأشياء الرئيسية والأشخاص الموجودين
2. مواقعهم والعلاقات المكانية
3. البيئة/الإعداد (داخلي/خارجي، الإضاءة، إلخ)
4. أي نص أو لافتات مرئية
5. المخاطر المحتملة أو معلومات التنقل

قدم وصفاً طبيعياً ومحادثياً كما لو كنت تتحدث مباشرة مع شخص ضعيف البصر. كن محدداً ولكن مختصراً. ابدأ بـ "يمكنني أن أرى..." `,
              objectIdentification: `أنت VoiceVision، تساعد شخصاً ضعيف البصر على تحديد الأشياء. حلل هذه الصورة و:

1. اسرد جميع الأشياء التي يمكنك تحديدها بوضوح مع مواقعها التقريبية (يسار، وسط، يمين، أعلى، أسفل)
2. صف حجمها ولونها وحالتها إذا كان ذلك ذا صلة
3. اذكر أي تفاصيل مفيدة لشخص ضعيف البصر (أزرار، مقابض، نص على الأشياء، إلخ)
4. إذا رأيت أشخاصاً، صف موقعهم العام وأنشطتهم
5. ركز على المعلومات القابلة للتنفيذ

قدم إجابة واضحة ومنظمة تساعد الشخص على فهم ما يوجد في بيئته. ابدأ بـ "يمكنني تحديد..." `,
            },
            hi: {
              sceneAnalysis: `आप VoiceVision हैं, दृष्टिबाधित उपयोगकर्ताओं के लिए एक AI सहायक। इस छवि का विश्लेषण करें और जो आप देखते हैं उसका स्पष्ट, उपयोगी विवरण प्रदान करें। इन पर ध्यान दें:

1. मुख्य वस्तुएं और उपस्थित लोग
2. उनकी स्थितियां और स्थानिक संबंध
3. वातावरण/सेटिंग (इनडोर/आउटडोर, प्रकाश व्यवस्था, आदि)
4. कोई भी दिखाई देने वाला पाठ या संकेत
5. संभावित खतरे या नेवीगेशन जानकारी

प्राकृतिक, बातचीत जैसा विवरण प्रदान करें जैसे आप सीधे किसी दृष्टिबाधित व्यक्ति से बात कर रहे हों। विशिष्ट लेकिन संक्षिप्त रहें। "मैं देख सकता हूं..." से शुरू करें `,
              objectIdentification: `आप VoiceVision हैं, एक दृष्टिबाधित व्यक्ति की वस्तुओं की पहचान में मदद कर रहे हैं। इस छवि का विश्लेषण करें और:

1. उन सभी वस्तुओं की सूची बनाएं जिन्हें आप स्पष्ट रूप से पहचान सकते हैं उनकी अनुमानित स्थितियों के साथ (बाएं, केंद्र, दाएं, ऊपर, नीचे)
2. यदि प्रासंगिक हो तो उनका आकार, रंग और स्थिति का वर्णन करें
3. दृष्टिबाधित व्यक्ति के लिए किसी भी उपयोगी विवरण का उल्लेख करें (बटन, हैंडल, वस्तुओं पर पाठ, आदि)
4. यदि आप लोगों को देखते हैं, तो उनकी सामान्य स्थिति और गतिविधियों का वर्णन करें
5. कार्यनीति जानकारी पर ध्यान दें

एक स्पष्ट, व्यवस्थित उत्तर प्रदान करें जो व्यक्ति को उनके वातावरण में क्या है इसे समझने में मदद करे। "मैं पहचान सकता हूं..." से शुरू करें `,
            },
          };

          // Add more languages as needed - for now, others will fall back to English

          this.initializeLanguage();
          this.initializeVoices();
          this.initializeVoiceRecognition();
          this.initializeAI();
          this.setupEventListeners();
          this.initializeSignLanguageButton();
        }

        initializeSignLanguageButton() {
          const button = document.getElementById("sign-language-btn");
          if (button) {
            button.textContent = `🤟 Sign Language: ${
              this.signLanguageEnabled ? "ON" : "OFF"
            }`;
          }
        }

        initializeLanguage() {
          // Update language button text to show current language
          const languageBtn = document.getElementById("language-btn");
          if (languageBtn) {
            languageBtn.textContent = `🌍 ${
              this.languages[this.currentLanguage]
            }`;
          }
        }

        async updateTesseractLanguage() {
          // Map our language codes to Tesseract language codes
          const tesseractLangMap = {
            en: "eng",
            es: "spa",
            fr: "fra",
            de: "deu",
            it: "ita",
            pt: "por",
            ru: "rus",
            zh: "chi_sim",
            ja: "jpn",
            ko: "kor",
            ar: "ara",
            hi: "hin",
          };

          const tesseractLang = tesseractLangMap[this.currentLanguage] || "eng";

          // Only update if we have a different language
          if (this.tesseractWorker && tesseractLang !== "eng") {
            try {
              this.updateStatus("🔄 Updating OCR language...", true);
              await this.tesseractWorker.terminate();
              this.tesseractWorker = await Tesseract.createWorker(
                tesseractLang
              );
              this.updateStatus("✅ OCR language updated");
            } catch (error) {
              console.warn(
                "Could not update OCR language, falling back to English:",
                error
              );
              this.tesseractWorker = await Tesseract.createWorker("eng");
            }
          }
        }

        filterVoicesByLanguage() {
          if (!this.availableVoices || this.availableVoices.length === 0)
            return;

          // Language code mapping for voice filtering
          const voiceLangMap = {
            en: ["en", "en-"],
            es: ["es", "es-"],
            fr: ["fr", "fr-"],
            de: ["de", "de-"],
            it: ["it", "it-"],
            pt: ["pt", "pt-"],
            ru: ["ru", "ru-"],
            zh: ["zh", "zh-"],
            ja: ["ja", "ja-"],
            ko: ["ko", "ko-"],
            ar: ["ar", "ar-"],
            hi: ["hi", "hi-"],
          };

          const targetLangPrefixes = voiceLangMap[this.currentLanguage] || [
            "en",
            "en-",
          ];

          // Find voices that match the current language
          const matchingVoices = this.availableVoices.filter((voice) =>
            targetLangPrefixes.some((prefix) => voice.lang.startsWith(prefix))
          );

          // If we found matching voices, select the first one
          if (matchingVoices.length > 0) {
            this.selectedVoice = matchingVoices[0];
            localStorage.setItem("voicevision_voice", this.selectedVoice.name);
          }
        }

        async initializeAI() {
          try {
            this.updateStatus(
              "🤖 Loading AI models (TensorFlow.js, Tesseract, Gemma 3n)...",
              true
            );

            // Initialize configuration - wait for it to be ready
            let config = window.voiceVisionConfig;
            if (!config) {
              console.log("⏳ Waiting for VoiceVision config to initialize...");
              // Wait up to 3 seconds for config to be ready
              for (let i = 0; i < 30; i++) {
                await new Promise(resolve => setTimeout(resolve, 100));
                config = window.voiceVisionConfig;
                if (config) break;
              }
              if (!config) {
                console.log("🆘 Config not ready, creating new instance");
                config = new VoiceVisionConfig();
              }
            }
            
            // Get API key from configuration
            let apiKey;
            try {
              apiKey = config.getGoogleAIKey();
            } catch (error) {
              console.error('API Key configuration error:', error.message);
              this.updateStatus(
                "⚠️ Google AI API key not configured - some features may be limited"
              );
              
              // Continue with other models even if Gemini is not available
              apiKey = null;
            }

            // Initialize Google Generative AI with Gemma 3n only if API key is available
            if (apiKey) {
              this.genAI = new GoogleGenerativeAI(apiKey);
              this.gemmaModel = this.genAI.getGenerativeModel({
                model: config.get('AI_MODEL') || "gemini-1.5-flash",
                generationConfig: {
                  temperature: config.get('AI_TEMPERATURE') || 0.7,
                  maxOutputTokens: config.get('AI_MAX_TOKENS') || 1024,
                },
              });
              console.log("✅ Gemma 3n AI model loaded");
            } else {
              console.warn("⚠️ Gemma 3n AI model not available - API key not configured");
              this.gemmaModel = null;
            }

            // Load TensorFlow COCO-SSD model for object detection with WebGL fallback
            try {
              // Check if WebGL is available
              const canvas = document.createElement('canvas');
              const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
              
              if (!gl) {
                console.warn("⚠️ WebGL not available, using CPU backend for TensorFlow.js");
                await tf.setBackend('cpu');
              }
              
              this.cocoModel = await cocoSsd.load();
              console.log("✅ Object detection model loaded");
            } catch (error) {
              console.warn("⚠️ Failed to load TensorFlow model:", error.message);
              console.warn("Object detection will be disabled");
              this.cocoModel = null;
            }

            // Initialize Tesseract worker for OCR
            this.tesseractWorker = await Tesseract.createWorker("eng");
            console.log("✅ OCR engine loaded");

            // Initialize MediaPipe Hands for sign language detection
            if (this.signLanguageEnabled) {
              await this.initializeSignLanguage();
            }

            this.updateStatus("🎯 All AI models loaded successfully!");
          } catch (error) {
            console.error("AI initialization error:", error);
            this.updateStatus(
              "⚠️ Some AI models failed to load - continuing with available models"
            );
          }
        }

        initializeVoices() {
          // Load saved voice settings
          const savedSpeed = localStorage.getItem("voicevision_speed");
          if (savedSpeed) {
            this.voiceSettings.rate = parseFloat(savedSpeed);
          }

          const savedVolume = localStorage.getItem("voicevision_volume");
          if (savedVolume) {
            this.voiceSettings.volume = parseFloat(savedVolume);
          }

          // Load available voices when they become ready
          if ("speechSynthesis" in window) {
            const loadVoices = () => {
              this.availableVoices = speechSynthesis.getVoices();

              // Try to set a good default voice
              const savedVoice = localStorage.getItem("voicevision_voice");
              if (savedVoice && this.availableVoices.length > 0) {
                this.selectedVoice = this.availableVoices.find(
                  (v) => v.name === savedVoice
                );
              }

              // If no saved voice or not found, pick a good default
              if (!this.selectedVoice && this.availableVoices.length > 0) {
                // Prefer English voices
                const englishVoices = this.availableVoices.filter(
                  (v) => v.lang.startsWith("en-") && !v.name.includes("Google")
                );

                // Look for natural sounding voices
                const preferredVoices = englishVoices.filter(
                  (v) =>
                    v.name.includes("Natural") ||
                    v.name.includes("Premium") ||
                    v.name.includes("Enhanced") ||
                    v.name.includes("Neural") ||
                    v.name.includes("Samantha") ||
                    v.name.includes("Alex") ||
                    v.name.includes("Victoria")
                );

                this.selectedVoice =
                  preferredVoices[0] ||
                  englishVoices[0] ||
                  this.availableVoices[0];
              }

              console.log(
                `✅ Voice system initialized with ${this.availableVoices.length} voices`
              );
              if (this.selectedVoice) {
                console.log(
                  `🎤 Selected voice: ${this.selectedVoice.name} (${this.selectedVoice.lang})`
                );
              }
            };

            // Load voices immediately if available
            loadVoices();

            // Also listen for voice changes (some browsers load voices asynchronously)
            speechSynthesis.onvoiceschanged = loadVoices;
          }
        }

        initializeVoiceRecognition() {
          // Initialize speech recognition for voice commands
          if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
            const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
            this.recognition = new SpeechRecognition();
            
            this.recognition.continuous = true;
            this.recognition.interimResults = false;
            this.recognition.lang = this.currentLanguage + '-US';
            
            this.recognition.onstart = () => {
              console.log('Voice recognition started');
            };
            
            this.recognition.onerror = (event) => {
              console.log('Voice recognition error:', event.error);
            };
            
            this.recognition.onend = () => {
              // Restart recognition if in settings mode
              if (this.voiceSelectionMode || this.languageSelectionMode) {
                setTimeout(() => {
                  if (this.voiceSelectionMode || this.languageSelectionMode) {
                    this.recognition.start();
                  }
                }, 100);
              }
            };
            
            console.log('✅ Voice recognition initialized');
          } else {
            console.log('Voice recognition not supported in this browser');
          }
        }

        setupEventListeners() {
          // Button event listeners
          document
            .getElementById("start-btn")
            .addEventListener("click", () => this.startCamera());
          document
            .getElementById("describe-btn")
            .addEventListener("click", () => this.describeScene());
          document
            .getElementById("read-btn")
            .addEventListener("click", () => this.readText());
          document
            .getElementById("objects-btn")
            .addEventListener("click", () => this.identifyObjects());
          document
            .getElementById("stop-btn")
            .addEventListener("click", () => this.stopCamera());
          document
            .getElementById("stop-speech-btn")
            .addEventListener("click", () => this.stopSpeech());
          document
            .getElementById("voice-select-btn")
            .addEventListener("click", () => this.showVoiceSettings());
          document
            .getElementById("language-btn")
            .addEventListener("click", () => this.showLanguageSettings());
          document
            .getElementById("sign-language-btn")
            .addEventListener("click", () => this.toggleSignLanguage());
          document
            .getElementById("api-key-btn")
            .addEventListener("click", () => this.updateAPIKey());

          // Keyboard shortcuts
          document.addEventListener("keydown", (e) => {
            if (!this.isRunning) return;

            switch (e.key.toLowerCase()) {
              case "d":
                this.describeScene();
                break;
              case "r":
                this.readText();
                break;
              case "o":
                this.identifyObjects();
                break;
              case "q":
                this.stopCamera();
                break;
              case "s":
                this.stopSpeech();
                break;
              case " ":
                e.preventDefault();
                if (this.lastDescription) {
                  this.speak(this.lastDescription);
                }
                break;
            }
          });

          // Window resize handler to update canvas overlay
          window.addEventListener("resize", () => {
            if (this.isRunning && this.handLandmarksCanvas) {
              setTimeout(() => this.resizeCanvasOverlay(), 100);
            }
          });
        }

        async startCamera() {
          try {
            this.updateStatus(
              "🔄 Starting camera and initializing AI systems...",
              true
            );

            // Request camera access
            this.stream = await navigator.mediaDevices.getUserMedia({
              video: {
                width: { ideal: 1280 },
                height: { ideal: 720 },
                facingMode: "environment",
              },
              audio: false,
            });

            this.video.srcObject = this.stream;

            await new Promise((resolve) => {
              this.video.onloadedmetadata = resolve;
            });

            // Wait for video to actually start playing to get accurate dimensions
            await new Promise((resolve) => {
              this.video.onplaying = resolve;
              this.video.play();
            });

            // Set up canvas overlay for hand landmarks to match displayed video size
            this.resizeCanvasOverlay();

            // Start hand tracking if sign language is enabled
            if (this.signLanguageEnabled && this.handsModel) {
              this.startHandTracking();
            }

            this.isRunning = true;
            this.updateStatus(
              "✅ VoiceVision is active! Camera and AI systems ready for assistance."
            );
            this.enableControls(true);

            const signLanguageStatus = this.signLanguageEnabled
              ? " Sign language detection is active."
              : "";
            this.speak(
              `VoiceVision camera and AI systems are now active. I can describe scenes, read text, or identify objects. Use the buttons or keyboard shortcuts.${signLanguageStatus}`
            );
          } catch (error) {
            console.error("Startup error:", error);
            this.updateStatus(
              "❌ Failed to access camera. Please allow camera permissions and try again."
            );
            this.speak(
              "Camera access failed. Please check your permissions and try again."
            );
          }
        }

        startHandTracking() {
          if (!this.handsModel || !this.signLanguageEnabled) return;

          const sendFrame = async () => {
            if (this.isRunning && this.signLanguageEnabled) {
              await this.handsModel.send({ image: this.video });
              requestAnimationFrame(sendFrame);
            }
          };

          requestAnimationFrame(sendFrame);
        }

        stopHandTracking() {
          // Clear the canvas
          if (this.handLandmarksCtx) {
            this.handLandmarksCtx.clearRect(
              0,
              0,
              this.handLandmarksCanvas.width,
              this.handLandmarksCanvas.height
            );
          }
          this.gestureBuffer = [];
        }

        resizeCanvasOverlay() {
          // Get the actual displayed dimensions of the video element
          const videoRect = this.video.getBoundingClientRect();
          const videoComputedStyle = window.getComputedStyle(this.video);

          // Account for borders
          const borderWidth = parseInt(videoComputedStyle.borderWidth) || 3;

          // Set canvas dimensions to match the displayed video size
          this.handLandmarksCanvas.width = videoRect.width - borderWidth * 2;
          this.handLandmarksCanvas.height = videoRect.height - borderWidth * 2;

          // Set canvas CSS dimensions to match
          this.handLandmarksCanvas.style.width = `${
            videoRect.width - borderWidth * 2
          }px`;
          this.handLandmarksCanvas.style.height = `${
            videoRect.height - borderWidth * 2
          }px`;

          console.log(
            `Canvas resized to: ${this.handLandmarksCanvas.width}x${this.handLandmarksCanvas.height}`
          );
        }

        async describeScene() {
          if (!this.isRunning) return;

          try {
            this.updateStatus("🧠 Analyzing scene with Gemma 3n AI...", true);

            // Capture current frame for analysis
            const canvas = this.captureFrame();
            const imageDataUrl = canvas.toDataURL("image/jpeg", 0.8);

            let description = "";

            if (this.gemmaModel) {
              // Use Gemma 3n for intelligent scene analysis
              description = await this.getGemmaSceneDescription(imageDataUrl);
            } else {
              // Fallback to basic analysis
              description = await this.getBasicSceneDescription(canvas);
            }

            this.lastDescription = description;
            this.updateStatus(`🔊 Gemma 3n Analysis: "${description}"`);
            this.speak(description);
          } catch (error) {
            console.error("Scene analysis error:", error);
            this.updateStatus("❌ Error analyzing scene");
            this.speak(
              "Sorry, I encountered an error while analyzing the scene."
            );
          }
        }

        async getGemmaSceneDescription(imageDataUrl) {
          try {
            // Convert image to format Gemma can understand
            const imageData = {
              inlineData: {
                data: imageDataUrl.split(",")[1], // Remove data:image/jpeg;base64, prefix
                mimeType: "image/jpeg",
              },
            };

            const prompt = this.getSceneAnalysisPrompt();

            const result = await this.gemmaModel.generateContent([
              prompt,
              imageData,
            ]);
            const response = await result.response;
            const description = response.text();

            return (
              description ||
              "I can see a scene but I'm having trouble describing it clearly."
            );
          } catch (error) {
            console.error("Gemma analysis error:", error);
            // Fallback to basic analysis
            return await this.getBasicSceneDescription();
          }
        }

        async getBasicSceneDescription(canvas) {
          // Fallback analysis using TensorFlow.js + basic image processing
          let description = "I can see ";

          if (this.cocoModel) {
            const predictions = await this.cocoModel.detect(canvas);

            if (predictions.length > 0) {
              const objects = predictions
                .filter((p) => p.score > 0.5)
                .map(
                  (p) => `${p.class} (${Math.round(p.score * 100)}% confidence)`
                )
                .slice(0, 5);

              if (objects.length > 0) {
                description += objects.join(", ") + ". ";
                description += this.analyzeSceneComposition(predictions);
              } else {
                description +=
                  "some objects but they're not clearly identifiable. ";
              }
            } else {
              description += "a scene but no specific objects are detected. ";
            }
          } else {
            description += "a scene. Basic object detection is not available. ";
          }

          // Add lighting analysis
          if (canvas) {
            description += this.analyzeLightingAndEnvironment(canvas);
          }

          return description;
        }

        async readText() {
          if (!this.isRunning) return;

          try {
            this.updateStatus("📖 Processing text with real OCR...", true);

            const canvas = this.captureFrame();

            if (this.tesseractWorker) {
              // Use real Tesseract OCR
              const {
                data: { text },
              } = await this.tesseractWorker.recognize(canvas);

              const cleanText = text.trim();

              if (cleanText && cleanText.length > 2) {
                const result = `I found text: "${cleanText}"`;
                this.updateStatus(`📝 Text Reading: "${result}"`);
                this.speak(result);
              } else {
                const result =
                  "No readable text detected in the current view. Try pointing the camera directly at text with good lighting.";
                this.updateStatus(`📝 Text Reading: "${result}"`);
                this.speak(result);
              }
            } else {
              const result =
                "OCR engine not available. Please try again after initialization.";
              this.updateStatus(`📝 Text Reading: "${result}"`);
              this.speak(result);
            }
          } catch (error) {
            console.error("Text reading error:", error);
            this.updateStatus("❌ Error reading text");
            this.speak("Sorry, I encountered an error while reading text.");
          }
        }

        async identifyObjects() {
          if (!this.isRunning) return;

          try {
            this.updateStatus("🎯 Analyzing objects with Gemma 3n AI...", true);

            const canvas = this.captureFrame();
            const imageDataUrl = canvas.toDataURL("image/jpeg", 0.8);

            let result = "";

            if (this.gemmaModel) {
              // Use Gemma 3n for intelligent object identification
              result = await this.getGemmaObjectAnalysis(imageDataUrl);
            } else {
              // Fallback to TensorFlow.js object detection
              result = await this.getBasicObjectDetection(canvas);
            }

            this.updateStatus(`🎯 Gemma 3n Object Analysis: ${result}`);
            this.speak(result);
          } catch (error) {
            console.error("Object detection error:", error);
            this.updateStatus("❌ Error identifying objects");
            this.speak(
              "Sorry, I encountered an error while identifying objects."
            );
          }
        }

        async getGemmaObjectAnalysis(imageDataUrl) {
          try {
            const imageData = {
              inlineData: {
                data: imageDataUrl.split(",")[1],
                mimeType: "image/jpeg",
              },
            };

            const prompt = this.getObjectIdentificationPrompt();

            const result = await this.gemmaModel.generateContent([
              prompt,
              imageData,
            ]);
            const response = await result.response;
            const description = response.text();

            return (
              description ||
              "I can see objects but I'm having trouble identifying them clearly."
            );
          } catch (error) {
            console.error("Gemma object analysis error:", error);
            return await this.getBasicObjectDetection();
          }
        }

        async getBasicObjectDetection(canvas) {
          if (this.cocoModel) {
            const predictions = await this.cocoModel.detect(canvas);

            if (predictions.length > 0) {
              const confidentObjects = predictions
                .filter((p) => p.score > 0.3)
                .sort((a, b) => b.score - a.score)
                .slice(0, 10);

              if (confidentObjects.length > 0) {
                const objectList = confidentObjects.map(
                  (p) => `${p.class} (${Math.round(p.score * 100)}% confidence)`
                );

                return `I can identify ${
                  confidentObjects.length
                } objects: ${objectList.join(", ")}`;
              } else {
                return "I can see objects but cannot identify them with high confidence. Try adjusting lighting or camera angle.";
              }
            } else {
              return "No objects detected. Try pointing the camera at different objects or improving lighting.";
            }
          } else {
            return "Object detection not available. Please wait for initialization.";
          }
        }

        getSceneAnalysisPrompt() {
          const prompts = this.languagePrompts[this.currentLanguage];
          return prompts
            ? prompts.sceneAnalysis
            : this.languagePrompts.en.sceneAnalysis;
        }

        getObjectIdentificationPrompt() {
          const prompts = this.languagePrompts[this.currentLanguage];
          return prompts
            ? prompts.objectIdentification
            : this.languagePrompts.en.objectIdentification;
        }

        async stopCamera() {
          // Immediately stop any ongoing speech
          if ("speechSynthesis" in window) {
            speechSynthesis.cancel();
          }

          if (this.stream) {
            this.stream.getTracks().forEach((track) => track.stop());
            this.stream = null;
          }

          // Stop hand tracking
          this.stopHandTracking();

          // Clean up AI resources
          if (this.tesseractWorker) {
            await this.tesseractWorker.terminate();
            this.tesseractWorker = null;
          }

          this.isRunning = false;
          this.updateStatus(
            '📢 VoiceVision stopped. Click "Start Camera & AI" to begin again.'
          );
          this.enableControls(false);

          // Use a short delay before speaking the stop message to ensure previous speech is cancelled
          setTimeout(() => {
            this.speak("VoiceVision has been stopped.");
          }, 100);
        }

        captureFrame() {
          const canvas = document.createElement("canvas");
          canvas.width = this.video.videoWidth || 640;
          canvas.height = this.video.videoHeight || 480;

          const context = canvas.getContext("2d");
          context.drawImage(this.video, 0, 0, canvas.width, canvas.height);

          return canvas; // Return canvas element for AI processing
        }

        analyzeSceneComposition(predictions) {
          if (!predictions || predictions.length === 0) {
            return "The scene appears to have low visual complexity.";
          }

          const objectCount = predictions.filter((p) => p.score > 0.5).length;

          if (objectCount === 0) {
            return "The scene has minimal identifiable objects.";
          } else if (objectCount <= 2) {
            return "This is a simple scene with a few main objects.";
          } else if (objectCount <= 5) {
            return "This is a moderately complex scene with several objects.";
          } else {
            return "This is a complex scene with many objects and details.";
          }
        }

        analyzeLightingAndEnvironment(canvas) {
          const context = canvas.getContext("2d");
          const imageData = context.getImageData(
            0,
            0,
            canvas.width,
            canvas.height
          );
          const data = imageData.data;

          // Calculate average brightness
          let totalBrightness = 0;
          for (let i = 0; i < data.length; i += 4) {
            const r = data[i];
            const g = data[i + 1];
            const b = data[i + 2];
            totalBrightness += (r + g + b) / 3;
          }

          const avgBrightness = totalBrightness / (data.length / 4);

          let lightingDescription = "";
          if (avgBrightness < 60) {
            lightingDescription = "The lighting is quite dim.";
          } else if (avgBrightness < 120) {
            lightingDescription = "The lighting is moderate.";
          } else if (avgBrightness < 180) {
            lightingDescription = "The lighting is good.";
          } else {
            lightingDescription = "The lighting is very bright.";
          }

          return lightingDescription;
        }

        updateStatus(message, showLoading = false) {
          const loadingSpinner = showLoading
            ? '<span class="loading"></span>'
            : "";
          this.statusText.innerHTML = loadingSpinner + message;
          console.log(message);
        }

        enableControls(enabled) {
          const buttons = [
            "describe-btn",
            "read-btn",
            "objects-btn",
            "stop-btn",
          ];
          buttons.forEach((id) => {
            const btn = document.getElementById(id);
            if (btn) btn.disabled = !enabled;
          });

          const startBtn = document.getElementById("start-btn");
          if (startBtn) startBtn.disabled = enabled;
        }

        speak(text) {
          if ("speechSynthesis" in window) {
            // Cancel any ongoing speech first
            speechSynthesis.cancel();

            // Small delay to ensure cancellation is processed
            setTimeout(() => {
              const utterance = new SpeechSynthesisUtterance(text);

              // Apply voice settings
              utterance.rate = this.voiceSettings.rate;
              utterance.pitch = this.voiceSettings.pitch;
              utterance.volume = this.voiceSettings.volume;

              // Use selected voice if available
              if (this.selectedVoice) {
                utterance.voice = this.selectedVoice;
              }

              speechSynthesis.speak(utterance);
            }, 50);
          } else {
            console.log("🔊 Would speak:", text);
          }
        }

        stopSpeech() {
          if ("speechSynthesis" in window) {
            speechSynthesis.cancel();
          }
        }

        delay(ms) {
          return new Promise((resolve) => setTimeout(resolve, ms));
        }

        showVoiceSettings() {
          if (!this.availableVoices || this.availableVoices.length === 0) {
            alert(
              "🎤 Voice system not ready yet. Please try again in a moment."
            );
            return;
          }

          // Create voice options list
          let voiceOptions = "🎤 Available Voices:\n\n";
          this.availableVoices.forEach((voice, index) => {
            const isSelected =
              this.selectedVoice && voice.name === this.selectedVoice.name
                ? " ✓"
                : "";
            voiceOptions += `${index + 1}. ${voice.name} (${
              voice.lang
            })${isSelected}\n`;
          });

          voiceOptions += `\nCurrent Settings:
📈 Speed: ${this.voiceSettings.rate}
🎵 Pitch: ${this.voiceSettings.pitch}
🔊 Volume: ${this.voiceSettings.volume}

Enter the number of the voice you want to use:`;

          const selection = prompt(voiceOptions);

          if (selection) {
            const voiceIndex = parseInt(selection) - 1;
            if (voiceIndex >= 0 && voiceIndex < this.availableVoices.length) {
              this.selectedVoice = this.availableVoices[voiceIndex];
              localStorage.setItem(
                "voicevision_voice",
                this.selectedVoice.name
              );

              // Test the new voice
              this.speak(
                `Voice changed to ${this.selectedVoice.name}. This is how I sound now.`
              );

              // Ask for speed adjustment
              this.adjustVoiceSettings();
            } else {
              alert("❌ Invalid selection. Please try again.");
            }
          }
        }

        showLanguageSettings() {
          // Create language options list
          let languageOptions = "🌍 Available Languages:\n\n";
          Object.entries(this.languages).forEach(([code, name], index) => {
            const isSelected = this.currentLanguage === code ? " ✓" : "";
            languageOptions += `${index + 1}. ${name}${isSelected}\n`;
          });

          languageOptions += `\nCurrent Language: ${
            this.languages[this.currentLanguage]
          }\n\nEnter the number of the language you want to use:`;

          const selection = prompt(languageOptions);

          if (selection) {
            const languageIndex = parseInt(selection) - 1;
            const languageCodes = Object.keys(this.languages);

            if (languageIndex >= 0 && languageIndex < languageCodes.length) {
              const newLanguage = languageCodes[languageIndex];
              this.currentLanguage = newLanguage;
              localStorage.setItem("voicevision_language", newLanguage);

              // Update Tesseract language
              this.updateTesseractLanguage();

              // Filter voices by new language
              this.filterVoicesByLanguage();

              // Update UI
              this.initializeLanguage();

              // Test the new language
              const testMessages = {
                en: "Language changed to English. All AI responses will now be in English.",
                es: "Idioma cambiado a español. Todas las respuestas de IA ahora estarán en español.",
                fr: "Langue changée en français. Toutes les réponses IA seront maintenant en français.",
                de: "Sprache auf Deutsch geändert. Alle KI-Antworten werden jetzt auf Deutsch sein.",
                it: "Lingua cambiata in italiano. Tutte le risposte AI saranno ora in italiano.",
                pt: "Idioma alterado para português. Todas as respostas de IA agora estarão em português.",
                ru: "Язык изменен на русский. Все ответы ИИ теперь будут на русском языке.",
                zh: "语言已更改为中文。所有AI回复现在都将使用中文。",
                ja: "言語を日本語に変更しました。すべてのAI応答は日本語になります。",
                ko: "언어가 한국어로 변경되었습니다. 모든 AI 응답이 이제 한국어로 제공됩니다.",
                ar: "تم تغيير اللغة إلى العربية. ستكون جميع استجابات الذكاء الاصطناعي الآن باللغة العربية.",
                hi: "भाषा हिंदी में बदल दी गई है। अब सभी AI प्रतिक्रियाएं हिंदी में होंगी।",
              };

              this.speak(testMessages[newLanguage] || testMessages.en);
            } else {
              alert("❌ Invalid selection. Please try again.");
            }
          }
        }

        adjustVoiceSettings() {
          const speedInput = prompt(
            "🎤 Voice Speed Settings:\n\n" +
              "Current speed: " +
              this.voiceSettings.rate +
              "\n" +
              "Enter new speed (0.1 to 2.0, where 1.0 is normal):\n" +
              "• 0.5 = Very slow\n" +
              "• 0.8 = Slow\n" +
              "• 1.0 = Normal\n" +
              "• 1.2 = Fast\n" +
              "• 1.5 = Very fast\n\n" +
              "(Press Cancel to keep current settings)"
          );

          if (speedInput !== null) {
            const newSpeed = parseFloat(speedInput);
            if (newSpeed >= 0.1 && newSpeed <= 2.0) {
              this.voiceSettings.rate = newSpeed;
              localStorage.setItem("voicevision_speed", newSpeed.toString());
              this.speak(
                `Speed adjusted to ${newSpeed}. This is the new speaking speed.`
              );
            } else if (speedInput.trim() !== "") {
              alert("❌ Please enter a speed between 0.1 and 2.0");
            }
          }
        }

        async initializeSignLanguage() {
          try {
            this.updateStatus(
              "🤟 Initializing sign language detection...",
              true
            );

            // Initialize MediaPipe Hands
            this.handsModel = new Hands({
              locateFile: (file) => {
                return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
              },
            });

            this.handsModel.setOptions({
              maxNumHands: 2,
              modelComplexity: 1,
              minDetectionConfidence: 0.5,
              minTrackingConfidence: 0.5,
            });

            this.handsModel.onResults(this.onHandsResults.bind(this));

            console.log("✅ Sign language detection initialized");
          } catch (error) {
            console.error("Sign language initialization error:", error);
            this.updateStatus(
              "⚠️ Sign language detection failed to initialize"
            );
          }
        }

        onHandsResults(results) {
          if (!this.signLanguageEnabled || !this.isRunning) return;

          // Clear canvas
          this.handLandmarksCtx.clearRect(
            0,
            0,
            this.handLandmarksCanvas.width,
            this.handLandmarksCanvas.height
          );

          if (results.multiHandLandmarks) {
            // Draw hand landmarks
            for (const landmarks of results.multiHandLandmarks) {
              this.drawHandLandmarks(landmarks);
            }

            // Detect gestures
            this.detectGestures(results.multiHandLandmarks);
          }
        }

        drawHandLandmarks(landmarks) {
          // Hand landmarks drawing disabled for cleaner UI
          // Users can enable debug mode if needed
          return;

          const ctx = this.handLandmarksCtx;
          ctx.fillStyle = "#FF0000";
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 2;

          // Draw connections between landmarks
          const connections = [
            [0, 1],
            [1, 2],
            [2, 3],
            [3, 4], // thumb
            [0, 5],
            [5, 6],
            [6, 7],
            [7, 8], // index finger
            [5, 9],
            [9, 10],
            [10, 11],
            [11, 12], // middle finger
            [9, 13],
            [13, 14],
            [14, 15],
            [15, 16], // ring finger
            [13, 17],
            [17, 18],
            [18, 19],
            [19, 20], // pinky
            [0, 17], // palm
          ];

          // Draw connections
          ctx.beginPath();
          for (const [start, end] of connections) {
            const startPoint = landmarks[start];
            const endPoint = landmarks[end];
            ctx.moveTo(
              startPoint.x * this.handLandmarksCanvas.width,
              startPoint.y * this.handLandmarksCanvas.height
            );
            ctx.lineTo(
              endPoint.x * this.handLandmarksCanvas.width,
              endPoint.y * this.handLandmarksCanvas.height
            );
          }
          ctx.stroke();

          // Draw landmarks
          for (const landmark of landmarks) {
            ctx.beginPath();
            ctx.arc(
              landmark.x * this.handLandmarksCanvas.width,
              landmark.y * this.handLandmarksCanvas.height,
              3,
              0,
              2 * Math.PI
            );
            ctx.fill();
          }
        }

        detectGestures(multiHandLandmarks) {
          if (multiHandLandmarks.length === 0) return;

          const now = Date.now();
          if (now - this.lastGestureTime < 2000) return; // Throttle gesture detection

          for (const landmarks of multiHandLandmarks) {
            const gesture = this.classifyGesture(landmarks);
            if (gesture) {
              // Always allow pause/resume gesture
              if (gesture === "pause" || !this.systemPaused) {
                this.gestureBuffer.push(gesture);

                // If we have enough consistent gestures, execute the command
                if (this.gestureBuffer.length >= this.gestureThreshold) {
                  const mostCommonGesture = this.getMostCommonGesture();
                  if (mostCommonGesture) {
                    this.executeGestureCommand(mostCommonGesture);
                    this.lastGestureTime = now;
                    this.gestureBuffer = [];
                  }
                }

                // Keep buffer size manageable
                if (this.gestureBuffer.length > this.gestureThreshold * 2) {
                  this.gestureBuffer = this.gestureBuffer.slice(
                    -this.gestureThreshold
                  );
                }
              }
            }
          }
        }

        classifyGesture(landmarks) {
          // Simple gesture classification based on finger positions
          const fingerTips = [4, 8, 12, 16, 20]; // thumb, index, middle, ring, pinky tips
          const fingerMcps = [3, 6, 10, 14, 18]; // finger base joints

          // Check which fingers are extended
          const fingersUp = [];

          // Thumb (different logic due to orientation)
          if (landmarks[4].x > landmarks[3].x) {
            fingersUp.push(1);
          } else {
            fingersUp.push(0);
          }

          // Other fingers
          for (let i = 1; i < 5; i++) {
            if (landmarks[fingerTips[i]].y < landmarks[fingerMcps[i]].y) {
              fingersUp.push(1);
            } else {
              fingersUp.push(0);
            }
          }

          // Classify gestures based on finger patterns
          const pattern = fingersUp.join("");

          // If in voice or language settings mode, check for additional gestures
          if (this.voiceSelectionMode || this.languageSelectionMode) {
            const settingsGesture = this.classifySettingsGesture(pattern, landmarks);
            if (settingsGesture) return settingsGesture;
          }

          switch (pattern) {
            case "01000": // Index finger only - Describe Scene
              return "describe";
            case "01100": // Index and middle finger - Read Text
              return "read";
            case "01110": // Index, middle, ring finger - Navigation Help
              return "navigate";
            case "00001": // Pinky only - Identify Objects
              return "objects";
            case "00010": // Ring finger only - Voice Settings
              return "voice";
            case "00100": // Middle finger only - Language Settings
              return "language";
            case "11000": // Thumb and index - Zoom In/Brightness
              return "brightness";
            case "10100": // Thumb and middle - Emergency Help
              return "emergency";
            case "11100": // Thumb, index, middle - Take Photo
              return "photo";
            case "11110": // All except pinky - Volume Control
              return "volume";
            case "11111": // All fingers - Stop
              return "stop";
            case "10000": // Thumb only - Repeat last
              return "repeat";
            case "00000": // Closed fist - Pause/Resume
              return "pause";
            case "01010": // Index and ring finger - Help/Tutorial
              return "help";
            case "10010": // Thumb and ring finger - Save Description
              return "save";
            case "10001": // Thumb and pinky - Toggle Speech Speed
              return "speed";
            default:
              return null;
          }
        }

        classifySettingsGesture(pattern, landmarks) {
          // Additional gestures specific to settings navigation
          
          // Check for pointing gestures
          const isPointingUp = this.isPointingDirection(landmarks, 'up');
          const isPointingDown = this.isPointingDirection(landmarks, 'down');
          const isPointingLeft = this.isPointingDirection(landmarks, 'left');
          const isPointingRight = this.isPointingDirection(landmarks, 'right');
          const isThumbsUp = this.isThumbsUp(landmarks);
          const isThumbsDown = this.isThumbsDown(landmarks);
          
          if (this.voiceSelectionMode) {
            if (isThumbsUp) return "voice_faster";
            if (isThumbsDown) return "voice_slower";
            if (isPointingLeft) return "voice_previous";
            if (isPointingRight) return "voice_next";
            if (pattern === "01100") return "voice_test"; // Peace sign for test
            if (pattern === "00000") return "voice_exit"; // Closed fist to exit
          }
          
          if (this.languageSelectionMode) {
            if (isPointingUp) return "language_next";
            if (isPointingDown) return "language_previous";
            if (pattern === "01100") return "language_select"; // Peace sign to select
            if (pattern === "00000") return "language_exit"; // Closed fist to exit
          }
          
          return null;
        }

        isPointingDirection(landmarks, direction) {
          // Simple direction detection based on index finger orientation
          const indexTip = landmarks[8];
          const indexBase = landmarks[5];
          
          const deltaX = indexTip.x - indexBase.x;
          const deltaY = indexTip.y - indexBase.y;
          
          const threshold = 0.05;
          
          switch (direction) {
            case 'up': return deltaY < -threshold && Math.abs(deltaX) < threshold;
            case 'down': return deltaY > threshold && Math.abs(deltaX) < threshold;
            case 'left': return deltaX < -threshold && Math.abs(deltaY) < threshold;
            case 'right': return deltaX > threshold && Math.abs(deltaY) < threshold;
            default: return false;
          }
        }

        isThumbsUp(landmarks) {
          // Thumb up, other fingers down
          const thumbTip = landmarks[4];
          const thumbBase = landmarks[3];
          const indexTip = landmarks[8];
          const indexBase = landmarks[5];
          
          return (thumbTip.y < thumbBase.y) && (indexTip.y > indexBase.y);
        }

        isThumbsDown(landmarks) {
          // Thumb down, other fingers down
          const thumbTip = landmarks[4];
          const thumbBase = landmarks[3];
          const indexTip = landmarks[8];
          const indexBase = landmarks[5];
          
          return (thumbTip.y > thumbBase.y) && (indexTip.y > indexBase.y);
        }

        getMostCommonGesture() {
          if (this.gestureBuffer.length === 0) return null;

          const counts = {};
          for (const gesture of this.gestureBuffer) {
            counts[gesture] = (counts[gesture] || 0) + 1;
          }

          let maxCount = 0;
          let mostCommon = null;
          for (const [gesture, count] of Object.entries(counts)) {
            if (count > maxCount) {
              maxCount = count;
              mostCommon = gesture;
            }
          }

          // Require at least 60% consistency
          return maxCount >= this.gestureThreshold * 0.6 ? mostCommon : null;
        }

        executeGestureCommand(gesture) {
          const gestureMessages = {
            describe: "Hand gesture detected: Describing scene",
            read: "Hand gesture detected: Reading text",
            navigate: "Hand gesture detected: Navigation assistance",
            objects: "Hand gesture detected: Identifying objects",
            voice: "Hand gesture detected: Opening voice settings",
            language: "Hand gesture detected: Opening language settings",
            brightness: "Hand gesture detected: Adjusting brightness",
            emergency: "Hand gesture detected: Emergency help mode",
            photo: "Hand gesture detected: Taking photo for analysis",
            volume: "Hand gesture detected: Adjusting volume",
            stop: "Hand gesture detected: Stopping camera",
            repeat: "Hand gesture detected: Repeating last description",
            pause: "Hand gesture detected: Pausing system",
            help: "Hand gesture detected: Opening help tutorial",
            save: "Hand gesture detected: Saving current description",
            speed: "Hand gesture detected: Adjusting speech speed",
            voice_faster: "Voice settings: Increasing speed",
            voice_slower: "Voice settings: Decreasing speed",
            voice_previous: "Voice settings: Previous voice",
            voice_next: "Voice settings: Next voice",
            voice_test: "Voice settings: Testing current voice",
            voice_exit: "Voice settings: Exiting settings",
            language_next: "Language settings: Next language",
            language_previous: "Language settings: Previous language",
            language_select: "Language settings: Selecting current language",
            language_exit: "Language settings: Exiting settings",
          };

          const message = gestureMessages[gesture];
          if (message) {
            this.updateStatus(`🤟 ${message}`);
            this.speak(message);

            // Execute the corresponding action
            setTimeout(() => {
              switch (gesture) {
                case "describe":
                  this.describeScene();
                  break;
                case "read":
                  this.readText();
                  break;
                case "navigate":
                  this.provideNavigationHelp();
                  break;
                case "objects":
                  this.identifyObjects();
                  break;
                case "voice":
                  this.showVoiceSettings();
                  break;
                case "language":
                  this.showLanguageSettings();
                  break;
                case "brightness":
                  this.adjustBrightness();
                  break;
                case "emergency":
                  this.activateEmergencyMode();
                  break;
                case "photo":
                  this.takePhotoForAnalysis();
                  break;
                case "volume":
                  this.adjustVolume();
                  break;
                case "stop":
                  this.stopCamera();
                  break;
                case "repeat":
                  if (this.lastDescription) {
                    this.speak(this.lastDescription);
                  } else {
                    this.speak("No previous description to repeat");
                  }
                  break;
                case "pause":
                  this.pauseSystem();
                  break;
                case "help":
                  this.showHelpTutorial();
                  break;
                case "save":
                  this.saveCurrentDescription();
                  break;
                case "speed":
                  this.toggleSpeechSpeed();
                  break;
                // Voice settings gestures
                case "voice_faster":
                  this.adjustVoiceSpeed(0.1);
                  break;
                case "voice_slower":
                  this.adjustVoiceSpeed(-0.1);
                  break;
                case "voice_previous":
                  this.cycleVoice(-1);
                  break;
                case "voice_next":
                  this.cycleVoice(1);
                  break;
                case "voice_test":
                  this.testCurrentVoice();
                  break;
                case "voice_exit":
                  this.exitVoiceSettings();
                  break;
                // Language settings gestures
                case "language_next":
                  this.cycleLanguage(1);
                  break;
                case "language_previous":
                  this.cycleLanguage(-1);
                  break;
                case "language_select":
                  if (this.languageOptions && this.currentLanguageIndex >= 0) {
                    const selectedLanguage = this.languageOptions[this.currentLanguageIndex];
                    this.setLanguage(selectedLanguage);
                  }
                  break;
                case "language_exit":
                  this.exitLanguageSettings();
                  break;
              }
            }, 1000); // Small delay to allow gesture message to be heard
          }
        }

        provideNavigationHelp() {
          if (!this.isRunning) return;

          try {
            this.updateStatus(
              "🧭 Analyzing environment for navigation...",
              true
            );
            const canvas = this.captureFrame();

            // Simple navigation analysis based on scene complexity and objects
            const lightingInfo = this.analyzeLightingAndEnvironment(canvas);

            let navigationAdvice = `Navigation analysis: ${lightingInfo} `;

            if (this.cocoModel) {
              this.cocoModel.detect(canvas).then((predictions) => {
                const obstacles = predictions.filter(
                  (p) =>
                    p.score > 0.5 &&
                    [
                      "person",
                      "chair",
                      "table",
                      "car",
                      "bicycle",
                      "motorcycle",
                    ].includes(p.class)
                );

                if (obstacles.length > 0) {
                  navigationAdvice += `I detect ${
                    obstacles.length
                  } potential obstacles: ${obstacles
                    .map((o) => o.class)
                    .join(", ")}. Please navigate carefully.`;
                } else {
                  navigationAdvice += "Path appears clear of major obstacles.";
                }

                this.updateStatus(`🧭 ${navigationAdvice}`);
                this.speak(navigationAdvice);
              });
            } else {
              navigationAdvice +=
                "Navigation system ready but object detection not available.";
              this.updateStatus(`🧭 ${navigationAdvice}`);
              this.speak(navigationAdvice);
            }
          } catch (error) {
            console.error("Navigation help error:", error);
            this.speak("Navigation assistance temporarily unavailable.");
          }
        }

        showVoiceSettings() {
          if (this.isProcessing) return;
          
          // Cancel any ongoing speech first
          if ('speechSynthesis' in window) {
            speechSynthesis.cancel();
          }
          
          const currentSpeed = this.voiceSettings.rate;
          const currentVolume = this.voiceSettings.volume;
          const voiceCount = this.availableVoices.length;
          
          this.speak(`Voice settings: Current speed is ${currentSpeed} times normal, volume is ${Math.round(currentVolume * 100)} percent. Say "faster" or "slower" to adjust speed, "louder" or "quieter" for volume, "next voice" or "previous voice" to change voice, "test voice" to hear current selection. Use sign language: thumbs up for faster, thumbs down for slower, point left for previous voice, point right for next voice.`);
          
          // Enable voice selection mode
          this.voiceSelectionMode = true;
          this.voiceSelectionTimeout = setTimeout(() => {
            this.voiceSelectionMode = false;
            this.speak("Voice settings mode ended.");
          }, 30000); // 30 seconds timeout
          
          // Set up voice command recognition for voice settings
          this.setupVoiceSettingsCommands();
          
          // Start voice recognition
          if (this.recognition) {
            try {
              this.recognition.start();
            } catch (e) {
              console.log('Voice recognition already running');
            }
          }
          
          // Visual feedback
          const voiceBtn = document.getElementById('voice-select-btn');
          if (voiceBtn) {
            voiceBtn.style.background = 'linear-gradient(135deg, #28a745, #20c997)';
            voiceBtn.textContent = '🎤 Voice Settings Active';
          }
        }

        showLanguageSettings() {
          if (this.isProcessing) return;
          
          // Cancel any ongoing speech first
          if ('speechSynthesis' in window) {
            speechSynthesis.cancel();
          }
          
          const languageOptions = [
            "English", "Spanish", "French", "German", "Italian", 
            "Portuguese", "Russian", "Chinese", "Japanese", "Korean", "Arabic", "Hindi"
          ];
          
          const currentLanguage = this.currentLanguage;
          const currentIndex = languageOptions.findIndex(lang => lang.toLowerCase() === currentLanguage.toLowerCase());
          
          this.speak(`Current language is ${currentLanguage}. Say "next language" to cycle forward, "previous language" to cycle backward, or say a specific language name like "Spanish" or "French". You can also use sign language: point up for next, point down for previous, or use the peace sign to confirm current selection.`);
          
          // Enable language selection mode
          this.languageSelectionMode = true;
          this.currentLanguageIndex = currentIndex >= 0 ? currentIndex : 0;
          this.languageOptions = languageOptions;
          
          this.languageSelectionTimeout = setTimeout(() => {
            this.languageSelectionMode = false;
            this.speak("Language settings mode ended.");
          }, 30000); // 30 seconds timeout
          
          // Set up voice command recognition for language selection
          this.setupLanguageSettingsCommands();
          
          // Start voice recognition
          if (this.recognition) {
            try {
              this.recognition.start();
            } catch (e) {
              console.log('Voice recognition already running');
            }
          }
          
          // Visual feedback
          const langBtn = document.getElementById('language-btn');
          if (langBtn) {
            langBtn.style.background = 'linear-gradient(135deg, #28a745, #20c997)';
            langBtn.textContent = `🌍 Language: ${languageOptions[this.currentLanguageIndex]}`;
          }
        }

        setupVoiceSettingsCommands() {
          // Enhanced voice recognition for voice settings
          if (!this.recognition) return;
          
          const originalOnResult = this.recognition.onresult;
          
          this.recognition.onresult = (event) => {
            if (!this.voiceSelectionMode) {
              if (originalOnResult) originalOnResult(event);
              return;
            }
            
            const transcript = event.results[event.resultIndex][0].transcript.toLowerCase().trim();
            
            if (transcript.includes('faster') || transcript.includes('speed up')) {
              this.adjustVoiceSpeed(0.1);
            } else if (transcript.includes('slower') || transcript.includes('slow down')) {
              this.adjustVoiceSpeed(-0.1);
            } else if (transcript.includes('louder') || transcript.includes('volume up')) {
              this.adjustVoiceVolume(0.1);
            } else if (transcript.includes('quieter') || transcript.includes('volume down')) {
              this.adjustVoiceVolume(-0.1);
            } else if (transcript.includes('next voice')) {
              this.cycleVoice(1);
            } else if (transcript.includes('previous voice')) {
              this.cycleVoice(-1);
            } else if (transcript.includes('test voice')) {
              this.testCurrentVoice();
            } else if (transcript.includes('done') || transcript.includes('exit') || transcript.includes('close')) {
              this.exitVoiceSettings();
            }
          };
        }

        setupLanguageSettingsCommands() {
          // Enhanced voice recognition for language settings
          if (!this.recognition) return;
          
          const originalOnResult = this.recognition.onresult;
          
          this.recognition.onresult = (event) => {
            if (!this.languageSelectionMode) {
              if (originalOnResult) originalOnResult(event);
              return;
            }
            
            const transcript = event.results[event.resultIndex][0].transcript.toLowerCase().trim();
            
            if (transcript.includes('next language')) {
              this.cycleLanguage(1);
            } else if (transcript.includes('previous language')) {
              this.cycleLanguage(-1);
            } else if (transcript.includes('done') || transcript.includes('exit') || transcript.includes('close')) {
              this.exitLanguageSettings();
            } else {
              // Check for specific language names
              const requestedLanguage = this.languageOptions.find(lang => 
                transcript.includes(lang.toLowerCase())
              );
              if (requestedLanguage) {
                this.setLanguage(requestedLanguage);
              }
            }
          };
        }

        adjustVoiceSpeed(delta) {
          this.voiceSettings.rate = Math.max(0.1, Math.min(3.0, this.voiceSettings.rate + delta));
          localStorage.setItem('voicevision_speed', this.voiceSettings.rate.toString());
          this.speak(`Speed adjusted to ${this.voiceSettings.rate.toFixed(1)} times normal.`);
        }

        adjustVoiceVolume(delta) {
          this.voiceSettings.volume = Math.max(0.0, Math.min(1.0, this.voiceSettings.volume + delta));
          localStorage.setItem('voicevision_volume', this.voiceSettings.volume.toString());
          this.speak(`Volume adjusted to ${Math.round(this.voiceSettings.volume * 100)} percent.`);
        }

        cycleVoice(direction) {
          if (this.availableVoices.length === 0) {
            this.speak("No voices available.");
            return;
          }
          
          const currentIndex = this.availableVoices.findIndex(v => v === this.selectedVoice);
          let newIndex = currentIndex + direction;
          
          if (newIndex >= this.availableVoices.length) newIndex = 0;
          if (newIndex < 0) newIndex = this.availableVoices.length - 1;
          
          this.selectedVoice = this.availableVoices[newIndex];
          localStorage.setItem('voicevision_voice', this.selectedVoice.name);
          
          this.speak(`Voice changed to ${this.selectedVoice.name}`);
        }

        testCurrentVoice() {
          if (this.selectedVoice) {
            this.speak(`This is the current voice: ${this.selectedVoice.name}. How does it sound?`);
          } else {
            this.speak("No voice selected.");
          }
        }

        cycleLanguage(direction) {
          this.currentLanguageIndex += direction;
          
          if (this.currentLanguageIndex >= this.languageOptions.length) {
            this.currentLanguageIndex = 0;
          }
          if (this.currentLanguageIndex < 0) {
            this.currentLanguageIndex = this.languageOptions.length - 1;
          }
          
          const newLanguage = this.languageOptions[this.currentLanguageIndex];
          this.speak(`Language option: ${newLanguage}. Say "select" to choose this language.`);
          
          // Update button text
          const langBtn = document.getElementById('language-btn');
          if (langBtn) {
            langBtn.textContent = `🌍 Language: ${newLanguage}`;
          }
        }

        setLanguage(language) {
          this.currentLanguage = language.toLowerCase();
          localStorage.setItem('voicevision_language', this.currentLanguage);
          
          // Find and set appropriate voice for the language
          this.setVoiceForLanguage();
          
          this.speak(`Language changed to ${language}. Voice settings have been updated accordingly.`);
          this.exitLanguageSettings();
        }

        exitVoiceSettings() {
          this.voiceSelectionMode = false;
          if (this.voiceSelectionTimeout) {
            clearTimeout(this.voiceSelectionTimeout);
          }
          
          // Reset button appearance
          const voiceBtn = document.getElementById('voice-select-btn');
          if (voiceBtn) {
            voiceBtn.style.background = '';
            voiceBtn.textContent = '🎤 Voice Settings';
          }
          
          this.speak("Voice settings saved.");
        }

        exitLanguageSettings() {
          this.languageSelectionMode = false;
          if (this.languageSelectionTimeout) {
            clearTimeout(this.languageSelectionTimeout);
          }
          
          // Reset button appearance
          const langBtn = document.getElementById('language-btn');
          if (langBtn) {
            langBtn.style.background = '';
            langBtn.textContent = '🌍 Language';
          }
          
          this.speak("Language settings saved.");
        }

        adjustBrightness() {
          if (!this.isRunning) return;

          const canvas = this.captureFrame();
          const lightingInfo = this.analyzeLightingAndEnvironment(canvas);

          this.speak(
            `Current lighting assessment: ${lightingInfo} You may want to adjust your device screen brightness or move to better lighting.`
          );
        }

        activateEmergencyMode() {
          this.speak(
            "Emergency mode activated. VoiceVision will provide continuous scene monitoring. To deactivate, use the stop gesture or say stop."
          );

          if (this.isRunning) {
            // Start continuous monitoring
            this.emergencyMode = true;
            this.startEmergencyMonitoring();
          }
        }

        startEmergencyMonitoring() {
          if (!this.emergencyMode || !this.isRunning) return;

          // Describe scene every 10 seconds in emergency mode
          this.describeScene();

          setTimeout(() => {
            this.startEmergencyMonitoring();
          }, 10000);
        }

        takePhotoForAnalysis() {
          if (!this.isRunning) return;

          try {
            const canvas = this.captureFrame();
            const dataUrl = canvas.toDataURL("image/jpeg", 0.8);

            // Store the photo data for detailed analysis
            this.savedPhoto = dataUrl;
            this.speak(
              "Photo captured for detailed analysis. Analyzing now..."
            );

            // Perform comprehensive analysis
            setTimeout(() => {
              this.describeScene();
              setTimeout(() => this.identifyObjects(), 2000);
              setTimeout(() => this.readText(), 4000);
            }, 1000);
          } catch (error) {
            console.error("Photo capture error:", error);
            this.speak("Unable to capture photo for analysis.");
          }
        }

        adjustVolume() {
          // Cycle through volume levels
          const volumeLevels = [0.3, 0.5, 0.8, 1.0];
          const currentIndex = volumeLevels.indexOf(this.voiceSettings.volume);
          const nextIndex = (currentIndex + 1) % volumeLevels.length;

          this.voiceSettings.volume = volumeLevels[nextIndex];
          localStorage.setItem(
            "voicevision_volume",
            this.voiceSettings.volume.toString()
          );

          this.speak(
            `Volume adjusted to ${Math.round(
              this.voiceSettings.volume * 100
            )} percent.`
          );
        }

        pauseSystem() {
          if (this.isRunning) {
            this.systemPaused = !this.systemPaused;

            if (this.systemPaused) {
              this.speak(
                "VoiceVision paused. Use closed fist gesture again to resume."
              );
              this.updateStatus(
                "⏸️ System paused - use closed fist gesture to resume"
              );
            } else {
              this.speak("VoiceVision resumed and ready for commands.");
              this.updateStatus("▶️ System resumed and active");
            }
          }
        }

        showHelpTutorial() {
          const helpText = `VoiceVision sign language gestures: 
          Index finger for scene description, 
          Two fingers for reading text, 
          Three fingers for navigation help,
          Ring finger for voice settings,
          Middle finger for language settings,
          Thumb and index for brightness,
          Thumb and middle for emergency mode,
          Thumb index and middle for photo capture,
          All fingers except pinky for volume,
          All fingers to stop,
          Thumb only to repeat,
          Closed fist to pause,
          Index and ring for this help,
          Thumb and ring to save description,
          Thumb and pinky for speech speed.`;

          this.speak(helpText);
          this.updateStatus("🆘 Help tutorial playing...");
        }

        saveCurrentDescription() {
          if (this.lastDescription) {
            // Save to localStorage with timestamp
            const timestamp = new Date().toLocaleString();
            const savedDescriptions = JSON.parse(
              localStorage.getItem("voicevision_saved") || "[]"
            );

            savedDescriptions.push({
              description: this.lastDescription,
              timestamp: timestamp,
            });

            // Keep only last 10 descriptions
            if (savedDescriptions.length > 10) {
              savedDescriptions.shift();
            }

            localStorage.setItem(
              "voicevision_saved",
              JSON.stringify(savedDescriptions)
            );
            this.speak("Current description saved successfully.");
          } else {
            this.speak("No description available to save.");
          }
        }

        toggleSpeechSpeed() {
          // Cycle through speed levels
          const speedLevels = [0.5, 0.7, 0.9, 1.0, 1.2, 1.5];
          const currentIndex = speedLevels.indexOf(this.voiceSettings.rate);
          const nextIndex = (currentIndex + 1) % speedLevels.length;

          this.voiceSettings.rate = speedLevels[nextIndex];
          localStorage.setItem(
            "voicevision_speed",
            this.voiceSettings.rate.toString()
          );

          this.speak(
            `Speech speed adjusted to ${this.voiceSettings.rate} times normal speed.`
          );
        }

        toggleSignLanguage() {
          this.signLanguageEnabled = !this.signLanguageEnabled;
          localStorage.setItem(
            "voicevision_sign_language",
            this.signLanguageEnabled.toString()
          );

          const button = document.getElementById("sign-language-btn");
          if (button) {
            button.textContent = `🤟 Sign Language: ${
              this.signLanguageEnabled ? "ON" : "OFF"
            }`;
          }

          if (this.signLanguageEnabled) {
            this.initializeSignLanguage();
            this.speak(
              "Advanced sign language detection enabled. Available gestures: index finger for scene description, two fingers for reading text, three fingers for navigation, ring finger for voice settings, middle finger for language, thumb combinations for brightness and emergency mode, closed fist to pause, and many more. Use index and ring fingers together for complete gesture help."
            );
          } else {
            this.speak("Sign language detection disabled.");
            // Clear canvas
            if (this.handLandmarksCtx) {
              this.handLandmarksCtx.clearRect(
                0,
                0,
                this.handLandmarksCanvas.width,
                this.handLandmarksCanvas.height
              );
            }
          }
        }

        updateAPIKey() {
          try {
            // Try to use the global config instance first, or create a new one
            const config = window.voiceVisionConfig || new VoiceVisionConfig();
            const hasApiKey = !!config.get('GOOGLE_AI_API_KEY');
            const isDev = config.isDevelopment();
            
            let message = "🔑 API Key Configuration:\n\n";
            
            if (hasApiKey) {
              message += "✅ Google AI API key is configured and ready!\n";
              message += "All Gemma 3n AI features are available.\n\n";
            } else {
              message += "⚠️ Google AI API key is not configured.\n";
              message += "Some AI features may be limited.\n\n";
              
              if (isDev) {
                message += "To configure your API key:\n";
                message += "1. Get an API key from Google AI Studio\n";
                message += "2. Add it to your .env file:\n";
                message += "   GOOGLE_AI_API_KEY=your_api_key_here\n";
                message += "3. Reload the application\n\n";
              }
            }
            
            message += "Configuration Details:\n";
            message += `• Environment: ${config.get('NODE_ENV')}\n`;
            message += `• Debug Mode: ${config.get('DEBUG_MODE')}\n`;
            message += `• AI Model: ${config.get('AI_MODEL')}\n\n`;
            
            message += "All other AI models (OCR, Object Detection, Sign Language) ";
            message += "will work regardless of the API key configuration.";
            
            alert(message);
          } catch (error) {
            console.error('Error loading configuration:', error);
            
            // Fallback message if config loading fails
            const fallbackMessage = "🔑 API Key Configuration:\n\n" +
              "⚠️ Configuration system not fully loaded.\n" +
              "The application will use fallback settings.\n\n" +
              "Features available:\n" +
              "• OCR text reading\n" +
              "• Sign language recognition\n" +
              "• Basic object detection\n" +
              "• Voice controls\n\n" +
              "For full AI features, ensure your API key is properly configured.";
            
            alert(fallbackMessage);
          }
        }
      }

      // Initialize the interface when DOM is loaded
      document.addEventListener("DOMContentLoaded", () => {
        // Ensure configuration is available
        if (typeof VoiceVisionConfig === 'undefined') {
          console.warn('VoiceVisionConfig not loaded, some features may be limited');
        } else if (!window.voiceVisionConfig) {
          // Create global config instance if it doesn't exist
          window.voiceVisionConfig = new VoiceVisionConfig();
        }
        
        // Initialize the main interface
        new VoiceVisionInterface();
      });
    </script>
  </body>
</html>
