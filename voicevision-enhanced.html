ü¶´<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>VoiceVision - Gemma 3n AI Assistant</title>

    <!-- Add Tesseract.js for real OCR -->
    <script src="https://unpkg.com/tesseract.js@5.0.4/dist/tesseract.min.js"></script>
    <!-- Add TensorFlow.js for object detection -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.15.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>
    <!-- Add MediaPipe for sign language detection -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <!-- Add Google Generative AI for Gemma 3n -->
    <script type="importmap">
      {
        "imports": {
          "@google/generative-ai": "https://esm.run/@google/generative-ai"
        }
      }
    </script>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.6;
        color: #333;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        min-height: 100vh;
      }

      .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
      }

      .header {
        text-align: center;
        color: white;
        margin-bottom: 30px;
      }

      .header h1 {
        font-size: 2.5em;
        margin-bottom: 10px;
        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
      }

      .header p {
        font-size: 1.2em;
        opacity: 0.9;
      }

      .main-content {
        background: white;
        border-radius: 20px;
        padding: 30px;
        box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
        margin-bottom: 20px;
      }

      .camera-section {
        text-align: center;
        margin-bottom: 30px;
        position: relative;
        display: flex;
        justify-content: center;
        align-items: center;
        width: 100%;
      }

      #camera-feed {
        max-width: 100%;
        width: 100%;
        max-width: 640px;
        height: auto;
        aspect-ratio: 4/3;
        border-radius: 15px;
        box-shadow: 0 10px 25px rgba(0, 0, 0, 0.2);
        background: #f8f9fa;
        border: 3px solid #e9ecef;
        object-fit: cover;
      }

      #hand-landmarks {
        position: absolute;
        top: 3px;
        left: 50%;
        transform: translateX(-50%);
        border-radius: 15px;
        pointer-events: none;
        z-index: 10;
      }

      .controls {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 15px;
        margin: 30px 0;
      }

      .btn {
        padding: 15px 25px;
        font-size: 16px;
        font-weight: 600;
        border: none;
        border-radius: 10px;
        cursor: pointer;
        transition: all 0.3s ease;
        text-transform: uppercase;
        letter-spacing: 1px;
      }

      .btn:disabled {
        opacity: 0.6;
        cursor: not-allowed;
      }

      .btn-primary {
        background: linear-gradient(45deg, #007bff, #0056b3);
        color: white;
      }

      .btn-primary:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0, 123, 255, 0.4);
      }

      .btn-success {
        background: linear-gradient(45deg, #28a745, #1e7e34);
        color: white;
      }

      .btn-success:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(40, 167, 69, 0.4);
      }

      .btn-info {
        background: linear-gradient(45deg, #17a2b8, #117a8b);
        color: white;
      }

      .btn-info:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(23, 162, 184, 0.4);
      }

      .btn-warning {
        background: linear-gradient(45deg, #ffc107, #e0a800);
        color: #212529;
      }

      .btn-warning:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(255, 193, 7, 0.4);
      }

      .btn-danger {
        background: linear-gradient(45deg, #dc3545, #bd2130);
        color: white;
      }

      .btn-danger:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(220, 53, 69, 0.4);
      }

      .status-panel {
        background: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #007bff;
        margin: 20px 0;
        min-height: 120px;
      }

      .status-panel h3 {
        color: #007bff;
        margin-bottom: 10px;
      }

      .status-text {
        font-size: 16px;
        line-height: 1.5;
      }

      .loading {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 3px solid #f3f3f3;
        border-top: 3px solid #007bff;
        border-radius: 50%;
        animation: spin 1s linear infinite;
        margin-right: 10px;
      }

      @keyframes spin {
        0% {
          transform: rotate(0deg);
        }
        100% {
          transform: rotate(360deg);
        }
      }

      .features {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        gap: 20px;
        margin-top: 30px;
      }

      .feature-card {
        background: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        border-left: 4px solid #28a745;
      }

      .feature-card h4 {
        color: #28a745;
        margin-bottom: 10px;
      }

      .keyboard-shortcuts {
        background: #e9ecef;
        padding: 15px;
        border-radius: 10px;
        margin-top: 20px;
      }

      .keyboard-shortcuts h4 {
        margin-bottom: 10px;
        color: #495057;
      }

      .shortcut {
        display: inline-block;
        background: #6c757d;
        color: white;
        padding: 5px 10px;
        border-radius: 5px;
        margin: 5px;
        font-family: monospace;
      }

      @media (max-width: 768px) {
        .container {
          padding: 10px;
        }

        .header h1 {
          font-size: 2em;
        }

        .main-content {
          padding: 20px;
        }

        .controls {
          grid-template-columns: 1fr;
        }

        .camera-section {
          padding: 0 10px;
        }

        #camera-feed {
          width: 100%;
          max-width: 100%;
          height: auto;
        }

        #hand-landmarks {
          left: 50%;
          transform: translateX(-50%);
        }
      }

      @media (max-width: 480px) {
        .main-content {
          padding: 15px;
        }

        .camera-section {
          padding: 0 5px;
        }

        .btn {
          padding: 12px 20px;
          font-size: 14px;
        }
      }

      .accessibility-notice {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 15px;
        border-radius: 10px;
        margin-bottom: 20px;
      }

      .accessibility-notice h4 {
        color: #856404;
        margin-bottom: 10px;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="header">
        <h1>üéØ VoiceVision</h1>
        <p>
          Powered by Gemma 3n Multimodal AI - Real-Time Assistant for Visually
          Impaired
        </p>
      </div>

      <div class="accessibility-notice">
        <h4>‚ôø Accessibility Features</h4>
        <p>
          This application provides voice-guided navigation, real-time scene
          description, text reading, and object identification. All features are
          keyboard accessible and include audio feedback.
        </p>
      </div>

      <div class="main-content">
        <div class="camera-section">
          <video id="camera-feed" autoplay playsinline muted></video>
          <canvas
            id="hand-landmarks"
            style="
              position: absolute;
              top: 0;
              left: 0;
              pointer-events: none;
              z-index: 10;
            "
          ></canvas>
        </div>

        <div class="controls">
          <button id="start-btn" class="btn btn-primary">
            üì∑ Start Camera & AI
          </button>
          <button id="describe-btn" class="btn btn-success" disabled>
            üîç Describe Scene
          </button>
          <button id="read-btn" class="btn btn-info" disabled>
            üìñ Read Text (OCR)
          </button>
          <button id="objects-btn" class="btn btn-warning" disabled>
            üéØ Identify Objects
          </button>
          <button id="stop-btn" class="btn btn-danger" disabled>
            üõë Stop Camera
          </button>
          <button id="stop-speech-btn" class="btn btn-secondary">
            üîá Stop Speech
          </button>
          <button id="voice-select-btn" class="btn btn-secondary">
            üé§ Voice Settings
          </button>
          <button id="language-btn" class="btn btn-secondary">
            üåç Language
          </button>
          <button id="sign-language-btn" class="btn btn-secondary">
            ü§ü Sign Language: OFF
          </button>
          <button id="api-key-btn" class="btn btn-secondary">
            ‚ÑπÔ∏è API Info
          </button>
        </div>

        <div class="status-panel">
          <h3>üì¢ System Status</h3>
          <div id="status-text" class="status-text">
            Ready to start. Click "Start Camera & AI" to begin VoiceVision
            assistance.
          </div>
        </div>

        <div class="keyboard-shortcuts">
          <h4>‚å®Ô∏è Keyboard Shortcuts & Sign Language</h4>
          <span class="shortcut">D</span> Describe Scene
          <span class="shortcut">R</span> Read Text
          <span class="shortcut">O</span> Identify Objects
          <span class="shortcut">Q</span> Stop Camera
          <span class="shortcut">S</span> Stop Speech
          <span class="shortcut">Space</span> Repeat Last Description
          <br /><br />
          <strong>ü§ü Sign Language Gestures:</strong><br />
          <span class="shortcut">üëÜ</span> Index finger - Describe Scene<br />
          <span class="shortcut">‚úåÔ∏è</span> Two fingers - Read Text<br />
          <span class="shortcut">ÔøΩ</span> Three fingers - Navigation Help<br />
          <span class="shortcut">ÔøΩü§ô</span> Pinky finger - Identify Objects<br />
          <span class="shortcut">üíç</span> Ring finger - Voice Settings<br />
          <span class="shortcut">üñï</span> Middle finger - Language Settings<br />
          <span class="shortcut">ü§è</span> Thumb + Index - Brightness<br />
          <span class="shortcut">ü§å</span> Thumb + Middle - Emergency Mode<br />
          <span class="shortcut">üì∏</span> Thumb + Index + Middle - Take
          Photo<br />
          <span class="shortcut">üîä</span> 4 fingers - Volume Control<br />
          <span class="shortcut">‚úã</span> All fingers - Stop Camera<br />
          <span class="shortcut">üëç</span> Thumb only - Repeat Last<br />
          <span class="shortcut">‚úä</span> Closed fist - Pause/Resume<br />
          <span class="shortcut">ü§ò</span> Index + Ring - Help Tutorial<br />
          <span class="shortcut">ü§ô</span> Thumb + Pinky - Speech Speed
        </div>

        <div class="features">
          <div class="feature-card">
            <h4>üß† Gemma 3n Multimodal AI</h4>
            <p>
              Advanced AI provides intelligent scene understanding, natural
              language descriptions, and contextual awareness beyond basic
              object detection.
            </p>
          </div>
          <div class="feature-card">
            <h4>üîç Real-Time Scene Analysis</h4>
            <p>
              Combines TensorFlow.js object detection with Gemma 3n's
              intelligence to provide detailed, conversational scene
              descriptions with spatial awareness.
            </p>
          </div>
          <div class="feature-card">
            <h4>üìñ Optical Character Recognition</h4>
            <p>
              Tesseract.js OCR engine reads text from signs, documents, menus,
              and labels with high accuracy, even in challenging lighting
              conditions.
            </p>
          </div>
          <div class="feature-card">
            <h4>üéØ Intelligent Object Detection</h4>
            <p>
              Gemma 3n enhances object recognition with contextual
              understanding, providing detailed descriptions of objects, their
              purpose, and interaction possibilities.
            </p>
          </div>
          <div class="feature-card">
            <h4>üîä Voice Feedback</h4>
            <p>
              All analysis results are provided through clear voice synthesis,
              allowing hands-free operation and seamless accessibility.
            </p>
          </div>
          <div class="feature-card">
            <h4>ü§ü Sign Language Support</h4>
            <p>
              Real-time hand tracking and gesture recognition enables deaf-blind
              users to interact through sign language gestures for common
              commands like "describe scene", "read text", and "identify
              objects".
            </p>
          </div>
        </div>
      </div>
    </div>

    <script type="module">
      // Import Google Generative AI for Gemma 3n
      import { GoogleGenerativeAI } from "@google/generative-ai";

      // VoiceVision with Gemma 3n Integration
      class VoiceVisionInterface {
        constructor() {
          this.video = document.getElementById("camera-feed");
          this.statusText = document.getElementById("status-text");
          this.stream = null;
          this.isRunning = false;
          this.lastDescription = "";

          // AI Models
          this.cocoModel = null; // TensorFlow COCO-SSD for fast object detection
          this.tesseractWorker = null; // Tesseract for OCR
          this.gemmaModel = null; // Gemma 3n for intelligent scene understanding
          this.genAI = null; // Google Generative AI client

          // Voice Settings
          this.availableVoices = [];
          this.selectedVoice = null;
          this.voiceSettings = {
            rate: 0.9,
            pitch: 1.0,
            volume: 0.8,
          };

          // Sign Language Settings
          this.signLanguageEnabled =
            localStorage.getItem("voicevision_sign_language") === "true" ||
            false;
          this.handsModel = null;
          this.handLandmarksCanvas = document.getElementById("hand-landmarks");
          this.handLandmarksCtx = this.handLandmarksCanvas.getContext("2d");
          this.lastGestureTime = 0;
          this.gestureBuffer = [];
          this.gestureThreshold = 30; // frames to confirm gesture

          // System State
          this.emergencyMode = false;
          this.systemPaused = false;
          this.savedPhoto = null;

          // Language Settings
          this.currentLanguage =
            localStorage.getItem("voicevision_language") || "en";
          this.languages = {
            en: "English",
            es: "Espa√±ol",
            fr: "Fran√ßais",
            de: "Deutsch",
            it: "Italiano",
            pt: "Portugu√™s",
            ru: "–†—É—Å—Å–∫–∏–π",
            zh: "‰∏≠Êñá",
            ja: "Êó•Êú¨Ë™û",
            ko: "ÌïúÍµ≠Ïñ¥",
            ar: "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©",
            hi: "‡§π‡§ø‡§®‡•ç‡§¶‡•Ä",
          };

          // Language-specific prompts for Gemma 3n AI
          this.languagePrompts = {
            en: {
              sceneAnalysis: `You are VoiceVision, an AI assistant for visually impaired users. Analyze this image and provide a clear, helpful description of what you see. Focus on:

1. Main objects and people present
2. Their locations and spatial relationships  
3. The environment/setting (indoor/outdoor, lighting, etc.)
4. Any text or signs visible
5. Potential hazards or navigation information

Provide a natural, conversational description as if speaking directly to a visually impaired person. Be specific but concise. Start with "I can see..." `,
              objectIdentification: `You are VoiceVision, helping a visually impaired person identify objects. Analyze this image and:

1. List all objects you can clearly identify with their approximate locations (left, center, right, top, bottom)
2. Describe their size, color, and condition if relevant
3. Mention any useful details for a visually impaired person (buttons, handles, text on objects, etc.)
4. If you see people, describe their general position and activities
5. Focus on actionable information

Provide a clear, organized response that helps the person understand what's in their environment. Start with "I can identify..." `,
            },
            es: {
              sceneAnalysis: `Eres VoiceVision, un asistente de IA para usuarios con discapacidad visual. Analiza esta imagen y proporciona una descripci√≥n clara y √∫til de lo que ves. Enf√≥cate en:

1. Objetos principales y personas presentes
2. Sus ubicaciones y relaciones espaciales
3. El entorno/ambiente (interior/exterior, iluminaci√≥n, etc.)
4. Cualquier texto o se√±al visible
5. Peligros potenciales o informaci√≥n de navegaci√≥n

Proporciona una descripci√≥n natural y conversacional como si hablaras directamente con una persona con discapacidad visual. S√© espec√≠fico pero conciso. Comienza con "Puedo ver..." `,
              objectIdentification: `Eres VoiceVision, ayudando a una persona con discapacidad visual a identificar objetos. Analiza esta imagen y:

1. Lista todos los objetos que puedes identificar claramente con sus ubicaciones aproximadas (izquierda, centro, derecha, arriba, abajo)
2. Describe su tama√±o, color y condici√≥n si es relevante
3. Menciona cualquier detalle √∫til para una persona con discapacidad visual (botones, manijas, texto en objetos, etc.)
4. Si ves personas, describe su posici√≥n general y actividades
5. Enf√≥cate en informaci√≥n √∫til

Proporciona una respuesta clara y organizada que ayude a la persona a entender qu√© hay en su entorno. Comienza con "Puedo identificar..." `,
            },
            fr: {
              sceneAnalysis: `Vous √™tes VoiceVision, un assistant IA pour les utilisateurs malvoyants. Analysez cette image et fournissez une description claire et utile de ce que vous voyez. Concentrez-vous sur :

1. Les objets principaux et les personnes pr√©sentes
2. Leurs emplacements et relations spatiales
3. L'environnement/cadre (int√©rieur/ext√©rieur, √©clairage, etc.)
4. Tout texte ou panneau visible
5. Les dangers potentiels ou informations de navigation

Fournissez une description naturelle et conversationnelle comme si vous parliez directement √† une personne malvoyante. Soyez pr√©cis mais concis. Commencez par "Je peux voir..." `,
              objectIdentification: `Vous √™tes VoiceVision, aidant une personne malvoyante √† identifier des objets. Analysez cette image et :

1. Listez tous les objets que vous pouvez identifier clairement avec leurs emplacements approximatifs (gauche, centre, droite, haut, bas)
2. D√©crivez leur taille, couleur et condition si pertinent
3. Mentionnez tout d√©tail utile pour une personne malvoyante (boutons, poign√©es, texte sur objets, etc.)
4. Si vous voyez des personnes, d√©crivez leur position g√©n√©rale et activit√©s
5. Concentrez-vous sur l'information exploitable

Fournissez une r√©ponse claire et organis√©e qui aide la personne √† comprendre ce qui est dans son environnement. Commencez par "Je peux identifier..." `,
            },
            de: {
              sceneAnalysis: `Sie sind VoiceVision, ein KI-Assistent f√ºr sehbehinderte Benutzer. Analysieren Sie dieses Bild und geben Sie eine klare, hilfreiche Beschreibung dessen, was Sie sehen. Konzentrieren Sie sich auf:

1. Hauptobjekte und anwesende Personen
2. Ihre Standorte und r√§umlichen Beziehungen
3. Die Umgebung/das Setting (innen/au√üen, Beleuchtung, etc.)
4. Sichtbare Texte oder Schilder
5. Potenzielle Gefahren oder Navigationsinformationen

Geben Sie eine nat√ºrliche, gespr√§chige Beschreibung, als w√ºrden Sie direkt mit einer sehbehinderten Person sprechen. Seien Sie spezifisch aber pr√§gnant. Beginnen Sie mit "Ich kann sehen..." `,
              objectIdentification: `Sie sind VoiceVision und helfen einer sehbehinderten Person bei der Objektidentifikation. Analysieren Sie dieses Bild und:

1. Listen Sie alle Objekte auf, die Sie klar identifizieren k√∂nnen, mit ihren ungef√§hren Positionen (links, Mitte, rechts, oben, unten)
2. Beschreiben Sie ihre Gr√∂√üe, Farbe und Zustand, falls relevant
3. Erw√§hnen Sie n√ºtzliche Details f√ºr eine sehbehinderte Person (Kn√∂pfe, Griffe, Text auf Objekten, etc.)
4. Wenn Sie Personen sehen, beschreiben Sie ihre allgemeine Position und Aktivit√§ten
5. Konzentrieren Sie sich auf umsetzbare Informationen

Geben Sie eine klare, organisierte Antwort, die der Person hilft, zu verstehen, was in ihrer Umgebung ist. Beginnen Sie mit "Ich kann identifizieren..." `,
            },
            it: {
              sceneAnalysis: `Sei VoiceVision, un assistente IA per utenti ipovedenti. Analizza questa immagine e fornisci una descrizione chiara e utile di ci√≤ che vedi. Concentrati su:

1. Oggetti principali e persone presenti
2. Le loro posizioni e relazioni spaziali
3. L'ambiente/contesto (interno/esterno, illuminazione, ecc.)
4. Qualsiasi testo o cartello visibile
5. Pericoli potenziali o informazioni di navigazione

Fornisci una descrizione naturale e conversazionale come se stessi parlando direttamente con una persona ipovedente. Sii specifico ma conciso. Inizia con "Posso vedere..." `,
              objectIdentification: `Sei VoiceVision, aiuti una persona ipovedente a identificare oggetti. Analizza questa immagine e:

1. Elenca tutti gli oggetti che puoi identificare chiaramente con le loro posizioni approssimative (sinistra, centro, destra, alto, basso)
2. Descrivi le loro dimensioni, colore e condizioni se rilevanti
3. Menziona qualsiasi dettaglio utile per una persona ipovedente (pulsanti, maniglie, testo sugli oggetti, ecc.)
4. Se vedi persone, descrivi la loro posizione generale e attivit√†
5. Concentrati su informazioni utilizzabili

Fornisci una risposta chiara e organizzata che aiuti la persona a capire cosa c'√® nel loro ambiente. Inizia con "Posso identificare..." `,
            },
            pt: {
              sceneAnalysis: `Voc√™ √© VoiceVision, um assistente de IA para usu√°rios com defici√™ncia visual. Analise esta imagem e forne√ßa uma descri√ß√£o clara e √∫til do que voc√™ v√™. Foque em:

1. Objetos principais e pessoas presentes
2. Suas localiza√ß√µes e rela√ß√µes espaciais
3. O ambiente/configura√ß√£o (interno/externo, ilumina√ß√£o, etc.)
4. Qualquer texto ou sinal vis√≠vel
5. Perigos potenciais ou informa√ß√µes de navega√ß√£o

Forne√ßa uma descri√ß√£o natural e conversacional como se estivesse falando diretamente com uma pessoa com defici√™ncia visual. Seja espec√≠fico mas conciso. Comece com "Posso ver..." `,
              objectIdentification: `Voc√™ √© VoiceVision, ajudando uma pessoa com defici√™ncia visual a identificar objetos. Analise esta imagem e:

1. Liste todos os objetos que voc√™ pode identificar claramente com suas localiza√ß√µes aproximadas (esquerda, centro, direita, topo, baixo)
2. Descreva seu tamanho, cor e condi√ß√£o se relevante
3. Mencione qualquer detalhe √∫til para uma pessoa com defici√™ncia visual (bot√µes, al√ßas, texto em objetos, etc.)
4. Se voc√™ vir pessoas, descreva sua posi√ß√£o geral e atividades
5. Foque em informa√ß√µes acion√°veis

Forne√ßa uma resposta clara e organizada que ajude a pessoa a entender o que est√° em seu ambiente. Comece com "Posso identificar..." `,
            },
            ru: {
              sceneAnalysis: `–í—ã VoiceVision, –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Å–ª–∞–±–æ–≤–∏–¥—è—â–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ —á–µ—Ç–∫–æ–µ, –ø–æ–ª–µ–∑–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã –≤–∏–¥–∏—Ç–µ. –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å –Ω–∞:

1. –û—Å–Ω–æ–≤–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö –∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ª—é–¥—è—Ö
2. –ò—Ö —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö
3. –û–∫—Ä—É–∂–µ–Ω–∏–∏/–æ–±—Å—Ç–∞–Ω–æ–≤–∫–µ (–≤–Ω—É—Ç—Ä–∏/—Å–Ω–∞—Ä—É–∂–∏, –æ—Å–≤–µ—â–µ–Ω–∏–µ –∏ —Ç.–¥.)
4. –õ—é–±–æ–º –≤–∏–¥–∏–º–æ–º —Ç–µ–∫—Å—Ç–µ –∏–ª–∏ –∑–Ω–∞–∫–∞—Ö
5. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –æ–ø–∞—Å–Ω–æ—Å—Ç—è—Ö –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –∫–∞–∫ –µ—Å–ª–∏ –±—ã –≤—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å–ª–∞–±–æ–≤–∏–¥—è—â–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º. –ë—É–¥—å—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã, –Ω–æ –ª–∞–∫–æ–Ω–∏—á–Ω—ã. –ù–∞—á–Ω–∏—Ç–µ —Å "–Ø –≤–∏–∂—É..." `,
              objectIdentification: `–í—ã VoiceVision, –ø–æ–º–æ–≥–∞–µ—Ç–µ —Å–ª–∞–±–æ–≤–∏–¥—è—â–µ–º—É —á–µ–ª–æ–≤–µ–∫—É –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–∫—Ç—ã. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏:

1. –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –º–æ–∂–µ—Ç–µ —á–µ—Ç–∫–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –∏—Ö –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è–º–∏ (—Å–ª–µ–≤–∞, –≤ —Ü–µ–Ω—Ç—Ä–µ, —Å–ø—Ä–∞–≤–∞, –≤–≤–µ—Ä—Ö—É, –≤–Ω–∏–∑—É)
2. –û–ø–∏—à–∏—Ç–µ –∏—Ö —Ä–∞–∑–º–µ—Ä, —Ü–≤–µ—Ç –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ
3. –£–ø–æ–º—è–Ω–∏—Ç–µ –ª—é–±—ã–µ –ø–æ–ª–µ–∑–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –¥–ª—è —Å–ª–∞–±–æ–≤–∏–¥—è—â–µ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ (–∫–Ω–æ–ø–∫–∏, —Ä—É—á–∫–∏, —Ç–µ–∫—Å—Ç –Ω–∞ –æ–±—ä–µ–∫—Ç–∞—Ö –∏ —Ç.–¥.)
4. –ï—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ –ª—é–¥–µ–π, –æ–ø–∏—à–∏—Ç–µ –∏—Ö –æ–±—â–µ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å
5. –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ —á–µ—Ç–∫–∏–π, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–∂–µ—Ç —á–µ–ª–æ–≤–µ–∫—É –ø–æ–Ω—è—Ç—å, —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –µ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏–∏. –ù–∞—á–Ω–∏—Ç–µ —Å "–Ø –º–æ–≥—É –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å..." `,
            },
            zh: {
              sceneAnalysis: `ÊÇ®ÊòØVoiceVisionÔºå‰∏∫ËßÜÈöúÁî®Êà∑Êèê‰æõÊúçÂä°ÁöÑAIÂä©Êâã„ÄÇÂàÜÊûêËøôÂº†ÂõæÁâáÂπ∂Êèê‰æõÊÇ®ÊâÄÁúãÂà∞ÂÜÖÂÆπÁöÑÊ∏ÖÊô∞„ÄÅÊúâÁî®ÊèèËø∞„ÄÇÈáçÁÇπÂÖ≥Ê≥®Ôºö

1. ‰∏ªË¶ÅÁâ©‰ΩìÂíåÂú®Âú∫‰∫∫Âëò
2. ÂÆÉ‰ª¨ÁöÑ‰ΩçÁΩÆÂíåÁ©∫Èó¥ÂÖ≥Á≥ª
3. ÁéØÂ¢É/Âú∫ÊôØÔºàÂÆ§ÂÜÖ/ÂÆ§Â§ñ„ÄÅÁÖßÊòéÁ≠âÔºâ
4. ‰ªª‰ΩïÂèØËßÅÁöÑÊñáÂ≠óÊàñÊ†áÂøó
5. ÊΩúÂú®Âç±Èô©ÊàñÂØºËà™‰ø°ÊÅØ

Êèê‰æõËá™ÁÑ∂„ÄÅÂØπËØùÂºèÁöÑÊèèËø∞ÔºåÂ∞±ÂÉèÁõ¥Êé•‰∏éËßÜÈöú‰∫∫Â£´‰∫§Ë∞à‰∏ÄÊ†∑„ÄÇË¶ÅÂÖ∑‰Ωì‰ΩÜÁÆÄÊ¥Å„ÄÇ‰ª•"ÊàëÂèØ‰ª•ÁúãÂà∞..."ÂºÄÂßã `,
              objectIdentification: `ÊÇ®ÊòØVoiceVisionÔºåÂ∏ÆÂä©ËßÜÈöú‰∫∫Â£´ËØÜÂà´Áâ©‰Ωì„ÄÇÂàÜÊûêËøôÂº†ÂõæÁâáÂπ∂Ôºö

1. ÂàóÂá∫ÊÇ®ËÉΩÊ∏ÖÊ•öËØÜÂà´ÁöÑÊâÄÊúâÁâ©‰ΩìÂèäÂÖ∂Â§ßËá¥‰ΩçÁΩÆÔºàÂ∑¶‰æß„ÄÅ‰∏≠Â§Æ„ÄÅÂè≥‰æß„ÄÅ‰∏äÊñπ„ÄÅ‰∏ãÊñπÔºâ
2. Â¶ÇÁõ∏ÂÖ≥ÔºåÊèèËø∞ÂÆÉ‰ª¨ÁöÑÂ§ßÂ∞è„ÄÅÈ¢úËâ≤ÂíåÁä∂ÊÄÅ
3. ÊèêÂèäÂØπËßÜÈöú‰∫∫Â£´ÊúâÁî®ÁöÑ‰ªª‰ΩïÁªÜËäÇÔºàÊåâÈíÆ„ÄÅÊääÊâã„ÄÅÁâ©‰Ωì‰∏äÁöÑÊñáÂ≠óÁ≠âÔºâ
4. Â¶ÇÊûúÊÇ®ÁúãÂà∞‰∫∫ÔºåÊèèËø∞‰ªñ‰ª¨ÁöÑÂ§ßËá¥‰ΩçÁΩÆÂíåÊ¥ªÂä®
5. ‰∏ìÊ≥®‰∫éÂèØÊìç‰ΩúÁöÑ‰ø°ÊÅØ

Êèê‰æõÊ∏ÖÊô∞„ÄÅÊúâÊù°ÁêÜÁöÑÂõûÁ≠îÔºåÂ∏ÆÂä©Ëøô‰∏™‰∫∫ÁêÜËß£‰ªñ‰ª¨ÁéØÂ¢É‰∏≠ÁöÑÂÜÖÂÆπ„ÄÇ‰ª•"ÊàëÂèØ‰ª•ËØÜÂà´..."ÂºÄÂßã `,
            },
            ja: {
              sceneAnalysis: `„ÅÇ„Å™„Åü„ÅØVoiceVision„Åß„Åô„ÄÇË¶ñË¶öÈöúÂÆ≥ËÄÖ„ÅÆ„É¶„Éº„Ç∂„Éº„ÅÆ„Åü„ÇÅ„ÅÆAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ„Åì„ÅÆÁîªÂÉè„ÇíÂàÜÊûê„Åó„ÄÅË¶ã„Åà„Çã„ÇÇ„ÅÆ„ÅÆÊòéÁ¢∫„ÅßÂΩπÁ´ã„Å§Ë™¨Êòé„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ª•‰∏ã„Å´ÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„Å¶„Åè„Å†„Åï„ÅÑÔºö

1. ‰∏ªË¶Å„Å™Áâ©‰Ωì„Å®Â≠òÂú®„Åô„Çã‰∫∫„ÄÖ
2. „Åù„Çå„Çâ„ÅÆ‰ΩçÁΩÆ„Å®Á©∫ÈñìÁöÑÈñ¢‰øÇ
3. Áí∞Â¢É/Ë®≠ÂÆöÔºàÂ±ãÂÜÖ/Â±ãÂ§ñ„ÄÅÁÖßÊòé„Å™„Å©Ôºâ
4. Ë¶ã„Åà„ÇãÊñáÂ≠ó„ÇÑÊ®ôË≠ò
5. ÊΩúÂú®ÁöÑ„Å™Âç±Èô∫„ÇÑ„Éä„Éì„Ç≤„Éº„Ç∑„Éß„É≥ÊÉÖÂ†±

Ë¶ñË¶öÈöúÂÆ≥ËÄÖ„Å®Áõ¥Êé•Ë©±„Åô„Çà„ÅÜ„Å™Ëá™ÁÑ∂„Åß‰ºöË©±ÁöÑ„Å™Ë™¨Êòé„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂÖ∑‰ΩìÁöÑ„Å†„ÅåÁ∞°ÊΩî„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„ÄåÁßÅ„Å´„ÅØ...„ÅåË¶ã„Åà„Åæ„Åô„Äç„ÅßÂßã„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ `,
              objectIdentification: `„ÅÇ„Å™„Åü„ÅØVoiceVision„Åß„Åô„ÄÇË¶ñË¶öÈöúÂÆ≥ËÄÖ„ÅÆÁâ©‰ΩìË≠òÂà•„ÇíÊîØÊè¥„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„ÅÆÁîªÂÉè„ÇíÂàÜÊûê„ÅóÔºö

1. ÊòéÁ¢∫„Å´Ë≠òÂà•„Åß„Åç„Çã„Åô„Åπ„Å¶„ÅÆÁâ©‰Ωì„Çí„Åä„Åä„Çà„Åù„ÅÆ‰ΩçÁΩÆ„Å®„Å®„ÇÇ„Å´‰∏ÄË¶ßË°®Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÂ∑¶„ÄÅ‰∏≠Â§Æ„ÄÅÂè≥„ÄÅ‰∏ä„ÄÅ‰∏ãÔºâ
2. Èñ¢ÈÄ£„Åå„ÅÇ„Çå„Å∞„ÄÅ„Çµ„Ç§„Ç∫„ÄÅËâ≤„ÄÅÁä∂ÊÖã„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ
3. Ë¶ñË¶öÈöúÂÆ≥ËÄÖ„Å´„Å®„Å£„Å¶ÊúâÁî®„Å™Ë©≥Á¥∞„ÇíË®ÄÂèä„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà„Éú„Çø„É≥„ÄÅ„Éè„É≥„Éâ„É´„ÄÅÁâ©‰Ωì‰∏ä„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Å™„Å©Ôºâ
4. ‰∫∫„ÅåË¶ã„Åà„ÇãÂ†¥Âêà„ÄÅ„Åù„ÅÆ‰∏ÄËà¨ÁöÑ„Å™‰ΩçÁΩÆ„Å®Ê¥ªÂãï„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ
5. ÂÆüÁî®ÁöÑ„Å™ÊÉÖÂ†±„Å´ÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„Å¶„Åè„Å†„Åï„ÅÑ

„Åù„ÅÆ‰∫∫„ÅåÁí∞Â¢É„Å´„ÅÇ„Çã„ÇÇ„ÅÆ„ÇíÁêÜËß£„Åô„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§ÊòéÁ¢∫„ÅßÊï¥ÁêÜ„Åï„Çå„ÅüÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„ÄåÁßÅ„ÅØ...„ÇíË≠òÂà•„Åß„Åç„Åæ„Åô„Äç„ÅßÂßã„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ `,
            },
            ko: {
              sceneAnalysis: `ÎãπÏã†ÏùÄ ÏãúÍ∞Å Ïû•Ïï†Ïù∏ ÏÇ¨Ïö©ÏûêÎ•º ÏúÑÌïú AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Ïù∏ VoiceVisionÏûÖÎãàÎã§. Ïù¥ Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌïòÍ≥† Î≥¥Ïù¥Îäî Í≤ÉÏóê ÎåÄÌïú Î™ÖÌôïÌïòÍ≥† Ïú†Ïö©Ìïú ÏÑ§Î™ÖÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî. Îã§ÏùåÏóê ÏßëÏ§ëÌïòÏÑ∏Ïöî:

1. Ï£ºÏöî Î¨ºÏ≤¥ÏôÄ ÌòÑÏû¨ ÏûàÎäî ÏÇ¨ÎûåÎì§
2. Í∑∏Îì§Ïùò ÏúÑÏπòÏôÄ Í≥µÍ∞ÑÏ†Å Í¥ÄÍ≥Ñ
3. ÌôòÍ≤Ω/ÏÑ§Ï†ï (Ïã§ÎÇ¥/Ïã§Ïô∏, Ï°∞Î™Ö Îì±)
4. Î≥¥Ïù¥Îäî ÌÖçÏä§Ìä∏ÎÇò ÌëúÏßÄÌåê
5. Ïû†Ïû¨Ï†Å ÏúÑÌóòÏù¥ÎÇò ÎÇ¥ÎπÑÍ≤åÏù¥ÏÖò Ï†ïÎ≥¥

ÏãúÍ∞Å Ïû•Ïï†Ïù∏Í≥º ÏßÅÏ†ë ÎåÄÌôîÌïòÎäî Í≤ÉÏ≤òÎüº ÏûêÏó∞Ïä§ÎüΩÍ≥† ÎåÄÌôîÏ†ÅÏù∏ ÏÑ§Î™ÖÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî. Íµ¨Ï≤¥Ï†ÅÏù¥ÏßÄÎßå Í∞ÑÍ≤∞ÌïòÍ≤å ÌïòÏÑ∏Ïöî. "Ï†ÄÎäî ...ÏùÑ Î≥º Ïàò ÏûàÏäµÎãàÎã§"Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî `,
              objectIdentification: `ÎãπÏã†ÏùÄ ÏãúÍ∞Å Ïû•Ïï†Ïù∏Ïù¥ Î¨ºÏ≤¥Î•º ÏãùÎ≥ÑÌïòÎèÑÎ°ù ÎèïÎäî VoiceVisionÏûÖÎãàÎã§. Ïù¥ Ïù¥ÎØ∏ÏßÄÎ•º Î∂ÑÏÑùÌïòÍ≥†:

1. Î™ÖÌôïÌïòÍ≤å ÏãùÎ≥ÑÌï† Ïàò ÏûàÎäî Î™®Îì† Î¨ºÏ≤¥Î•º ÎåÄÎûµÏ†ÅÏù∏ ÏúÑÏπòÏôÄ Ìï®Íªò ÎÇòÏó¥ÌïòÏÑ∏Ïöî (ÏôºÏ™Ω, Ï§ëÏïô, Ïò§Î•∏Ï™Ω, ÏúÑ, ÏïÑÎûò)
2. Í¥ÄÎ†®Ïù¥ ÏûàÎã§Î©¥ ÌÅ¨Í∏∞, ÏÉâÏÉÅ, ÏÉÅÌÉúÎ•º ÏÑ§Î™ÖÌïòÏÑ∏Ïöî
3. ÏãúÍ∞Å Ïû•Ïï†Ïù∏ÏóêÍ≤å Ïú†Ïö©Ìïú ÏÑ∏Î∂ÄÏÇ¨Ìï≠ÏùÑ Ïñ∏Í∏âÌïòÏÑ∏Ïöî (Î≤ÑÌäº, ÏÜêÏû°Ïù¥, Î¨ºÏ≤¥Ïùò ÌÖçÏä§Ìä∏ Îì±)
4. ÏÇ¨ÎûåÏù¥ Î≥¥Ïù∏Îã§Î©¥ ÏùºÎ∞òÏ†ÅÏù∏ ÏúÑÏπòÏôÄ ÌôúÎèôÏùÑ ÏÑ§Î™ÖÌïòÏÑ∏Ïöî
5. Ïã§Ìñâ Í∞ÄÎä•Ìïú Ï†ïÎ≥¥Ïóê ÏßëÏ§ëÌïòÏÑ∏Ïöî

Í∑∏ ÏÇ¨ÎûåÏù¥ ÌôòÍ≤ΩÏóê Î¨¥ÏóáÏù¥ ÏûàÎäîÏßÄ Ïù¥Ìï¥ÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Î™ÖÌôïÌïòÍ≥† Ï≤¥Í≥ÑÏ†ÅÏù∏ ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî. "Ï†ÄÎäî ...ÏùÑ ÏãùÎ≥ÑÌï† Ïàò ÏûàÏäµÎãàÎã§"Î°ú ÏãúÏûëÌïòÏÑ∏Ïöî `,
            },
            ar: {
              sceneAnalysis: `ÿ£ŸÜÿ™ VoiceVisionÿå ŸÖÿ≥ÿßÿπÿØ ÿ∞ŸÉŸä ŸÑŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖŸäŸÜ ÿ∂ÿπÿßŸÅ ÿßŸÑÿ®ÿµÿ±. ÿ≠ŸÑŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿµŸàÿ±ÿ© ŸàŸÇÿØŸÖ ŸàÿµŸÅÿßŸã Ÿàÿßÿ∂ÿ≠ÿßŸã ŸàŸÖŸÅŸäÿØÿßŸã ŸÑŸÖÿß ÿ™ÿ±ÿßŸá. ÿ±ŸÉÿ≤ ÿπŸÑŸâ:

1. ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸàÿßŸÑÿ£ÿ¥ÿÆÿßÿµ ÿßŸÑŸÖŸàÿ¨ŸàÿØŸäŸÜ
2. ŸÖŸàÿßŸÇÿπŸáŸÖ ŸàÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿßŸÑŸÖŸÉÿßŸÜŸäÿ©
3. ÿßŸÑÿ®Ÿäÿ¶ÿ©/ÿßŸÑÿ•ÿπÿØÿßÿØ (ÿØÿßÿÆŸÑŸä/ÿÆÿßÿ±ÿ¨Ÿäÿå ÿßŸÑÿ•ÿ∂ÿßÿ°ÿ©ÿå ÿ•ŸÑÿÆ)
4. ÿ£Ÿä ŸÜÿµ ÿ£Ÿà ŸÑÿßŸÅÿ™ÿßÿ™ ŸÖÿ±ÿ¶Ÿäÿ©
5. ÿßŸÑŸÖÿÆÿßÿ∑ÿ± ÿßŸÑŸÖÿ≠ÿ™ŸÖŸÑÿ© ÿ£Ÿà ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ™ŸÜŸÇŸÑ

ŸÇÿØŸÖ ŸàÿµŸÅÿßŸã ÿ∑ÿ®ŸäÿπŸäÿßŸã ŸàŸÖÿ≠ÿßÿØÿ´ŸäÿßŸã ŸÉŸÖÿß ŸÑŸà ŸÉŸÜÿ™ ÿ™ÿ™ÿ≠ÿØÿ´ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÖÿπ ÿ¥ÿÆÿµ ÿ∂ÿπŸäŸÅ ÿßŸÑÿ®ÿµÿ±. ŸÉŸÜ ŸÖÿ≠ÿØÿØÿßŸã ŸàŸÑŸÉŸÜ ŸÖÿÆÿ™ÿµÿ±ÿßŸã. ÿßÿ®ÿØÿ£ ÿ®ŸÄ "ŸäŸÖŸÉŸÜŸÜŸä ÿ£ŸÜ ÿ£ÿ±Ÿâ..." `,
              objectIdentification: `ÿ£ŸÜÿ™ VoiceVisionÿå ÿ™ÿ≥ÿßÿπÿØ ÿ¥ÿÆÿµÿßŸã ÿ∂ÿπŸäŸÅ ÿßŸÑÿ®ÿµÿ± ÿπŸÑŸâ ÿ™ÿ≠ÿØŸäÿØ ÿßŸÑÿ£ÿ¥Ÿäÿßÿ°. ÿ≠ŸÑŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿµŸàÿ±ÿ© Ÿà:

1. ÿßÿ≥ÿ±ÿØ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ÿßŸÑÿ™Ÿä ŸäŸÖŸÉŸÜŸÉ ÿ™ÿ≠ÿØŸäÿØŸáÿß ÿ®Ÿàÿ∂Ÿàÿ≠ ŸÖÿπ ŸÖŸàÿßŸÇÿπŸáÿß ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿäÿ© (Ÿäÿ≥ÿßÿ±ÿå Ÿàÿ≥ÿ∑ÿå ŸäŸÖŸäŸÜÿå ÿ£ÿπŸÑŸâÿå ÿ£ÿ≥ŸÅŸÑ)
2. ÿµŸÅ ÿ≠ÿ¨ŸÖŸáÿß ŸàŸÑŸàŸÜŸáÿß Ÿàÿ≠ÿßŸÑÿ™Ÿáÿß ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿ∞ŸÑŸÉ ÿ∞ÿß ÿµŸÑÿ©
3. ÿßÿ∞ŸÉÿ± ÿ£Ÿä ÿ™ŸÅÿßÿµŸäŸÑ ŸÖŸÅŸäÿØÿ© ŸÑÿ¥ÿÆÿµ ÿ∂ÿπŸäŸÅ ÿßŸÑÿ®ÿµÿ± (ÿ£ÿ≤ÿ±ÿßÿ±ÿå ŸÖŸÇÿßÿ®ÿ∂ÿå ŸÜÿµ ÿπŸÑŸâ ÿßŸÑÿ£ÿ¥Ÿäÿßÿ°ÿå ÿ•ŸÑÿÆ)
4. ÿ•ÿ∞ÿß ÿ±ÿ£Ÿäÿ™ ÿ£ÿ¥ÿÆÿßÿµÿßŸãÿå ÿµŸÅ ŸÖŸàŸÇÿπŸáŸÖ ÿßŸÑÿπÿßŸÖ Ÿàÿ£ŸÜÿ¥ÿ∑ÿ™ŸáŸÖ
5. ÿ±ŸÉÿ≤ ÿπŸÑŸâ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑŸÇÿßÿ®ŸÑÿ© ŸÑŸÑÿ™ŸÜŸÅŸäÿ∞

ŸÇÿØŸÖ ÿ•ÿ¨ÿßÿ®ÿ© Ÿàÿßÿ∂ÿ≠ÿ© ŸàŸÖŸÜÿ∏ŸÖÿ© ÿ™ÿ≥ÿßÿπÿØ ÿßŸÑÿ¥ÿÆÿµ ÿπŸÑŸâ ŸÅŸáŸÖ ŸÖÿß ŸäŸàÿ¨ÿØ ŸÅŸä ÿ®Ÿäÿ¶ÿ™Ÿá. ÿßÿ®ÿØÿ£ ÿ®ŸÄ "ŸäŸÖŸÉŸÜŸÜŸä ÿ™ÿ≠ÿØŸäÿØ..." `,
            },
            hi: {
              sceneAnalysis: `‡§Ü‡§™ VoiceVision ‡§π‡•à‡§Ç, ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§¨‡§æ‡§ß‡§ø‡§§ ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï AI ‡§∏‡§π‡§æ‡§Ø‡§ï‡•§ ‡§á‡§∏ ‡§õ‡§µ‡§ø ‡§ï‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§ú‡•ã ‡§Ü‡§™ ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•à‡§Ç ‡§â‡§∏‡§ï‡§æ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü, ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§á‡§® ‡§™‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç:

1. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§µ‡§∏‡•ç‡§§‡•Å‡§è‡§Ç ‡§î‡§∞ ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§ ‡§≤‡•ã‡§ó
2. ‡§â‡§®‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡§æ‡§Ç ‡§î‡§∞ ‡§∏‡•ç‡§•‡§æ‡§®‡§ø‡§ï ‡§∏‡§Ç‡§¨‡§Ç‡§ß
3. ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£/‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó (‡§á‡§®‡§°‡•ã‡§∞/‡§Ü‡§â‡§ü‡§°‡•ã‡§∞, ‡§™‡•ç‡§∞‡§ï‡§æ‡§∂ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ, ‡§Ü‡§¶‡§ø)
4. ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§™‡§æ‡§† ‡§Ø‡§æ ‡§∏‡§Ç‡§ï‡•á‡§§
5. ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§ø‡§§ ‡§ñ‡§§‡§∞‡•á ‡§Ø‡§æ ‡§®‡•á‡§µ‡•Ä‡§ó‡•á‡§∂‡§® ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä

‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï, ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ú‡•à‡§∏‡§æ ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç ‡§ú‡•à‡§∏‡•á ‡§Ü‡§™ ‡§∏‡•Ä‡§ß‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§¨‡§æ‡§ß‡§ø‡§§ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§∏‡•á ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã‡§Ç‡•§ ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∞‡§π‡•á‡§Ç‡•§ "‡§Æ‡•à‡§Ç ‡§¶‡•á‡§ñ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç..." ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç `,
              objectIdentification: `‡§Ü‡§™ VoiceVision ‡§π‡•à‡§Ç, ‡§è‡§ï ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§¨‡§æ‡§ß‡§ø‡§§ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ï‡•Ä ‡§µ‡§∏‡•ç‡§§‡•Å‡§ì‡§Ç ‡§ï‡•Ä ‡§™‡§π‡§ö‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ ‡§á‡§∏ ‡§õ‡§µ‡§ø ‡§ï‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞:

1. ‡§â‡§® ‡§∏‡§≠‡•Ä ‡§µ‡§∏‡•ç‡§§‡•Å‡§ì‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§¨‡§®‡§æ‡§è‡§Ç ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§Ü‡§™ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡§π‡§ö‡§æ‡§® ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§â‡§®‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§®‡§ø‡§§ ‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• (‡§¨‡§æ‡§è‡§Ç, ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞, ‡§¶‡§æ‡§è‡§Ç, ‡§ä‡§™‡§∞, ‡§®‡•Ä‡§ö‡•á)
2. ‡§Ø‡§¶‡§ø ‡§™‡•ç‡§∞‡§æ‡§∏‡§Ç‡§ó‡§ø‡§ï ‡§π‡•ã ‡§§‡•ã ‡§â‡§®‡§ï‡§æ ‡§Ü‡§ï‡§æ‡§∞, ‡§∞‡§Ç‡§ó ‡§î‡§∞ ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç
3. ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø‡§¨‡§æ‡§ß‡§ø‡§§ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§ï‡§æ ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ ‡§ï‡§∞‡•á‡§Ç (‡§¨‡§ü‡§®, ‡§π‡•à‡§Ç‡§°‡§≤, ‡§µ‡§∏‡•ç‡§§‡•Å‡§ì‡§Ç ‡§™‡§∞ ‡§™‡§æ‡§†, ‡§Ü‡§¶‡§ø)
4. ‡§Ø‡§¶‡§ø ‡§Ü‡§™ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§â‡§®‡§ï‡•Ä ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§î‡§∞ ‡§ó‡§§‡§ø‡§µ‡§ø‡§ß‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç
5. ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§®‡•Ä‡§§‡§ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§™‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç

‡§è‡§ï ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü, ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç ‡§ú‡•ã ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ï‡•ã ‡§â‡§®‡§ï‡•á ‡§µ‡§æ‡§§‡§æ‡§µ‡§∞‡§£ ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à ‡§á‡§∏‡•á ‡§∏‡§Æ‡§ù‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡•á‡•§ "‡§Æ‡•à‡§Ç ‡§™‡§π‡§ö‡§æ‡§® ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Ç..." ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç `,
            },
          };

          // Add more languages as needed - for now, others will fall back to English

          this.initializeLanguage();
          this.initializeVoices();
          this.initializeAI();
          this.setupEventListeners();
          this.initializeSignLanguageButton();
        }

        initializeSignLanguageButton() {
          const button = document.getElementById("sign-language-btn");
          if (button) {
            button.textContent = `ü§ü Sign Language: ${
              this.signLanguageEnabled ? "ON" : "OFF"
            }`;
          }
        }

        initializeLanguage() {
          // Update language button text to show current language
          const languageBtn = document.getElementById("language-btn");
          if (languageBtn) {
            languageBtn.textContent = `üåç ${
              this.languages[this.currentLanguage]
            }`;
          }
        }

        async updateTesseractLanguage() {
          // Map our language codes to Tesseract language codes
          const tesseractLangMap = {
            en: "eng",
            es: "spa",
            fr: "fra",
            de: "deu",
            it: "ita",
            pt: "por",
            ru: "rus",
            zh: "chi_sim",
            ja: "jpn",
            ko: "kor",
            ar: "ara",
            hi: "hin",
          };

          const tesseractLang = tesseractLangMap[this.currentLanguage] || "eng";

          // Only update if we have a different language
          if (this.tesseractWorker && tesseractLang !== "eng") {
            try {
              this.updateStatus("üîÑ Updating OCR language...", true);
              await this.tesseractWorker.terminate();
              this.tesseractWorker = await Tesseract.createWorker(
                tesseractLang
              );
              this.updateStatus("‚úÖ OCR language updated");
            } catch (error) {
              console.warn(
                "Could not update OCR language, falling back to English:",
                error
              );
              this.tesseractWorker = await Tesseract.createWorker("eng");
            }
          }
        }

        filterVoicesByLanguage() {
          if (!this.availableVoices || this.availableVoices.length === 0)
            return;

          // Language code mapping for voice filtering
          const voiceLangMap = {
            en: ["en", "en-"],
            es: ["es", "es-"],
            fr: ["fr", "fr-"],
            de: ["de", "de-"],
            it: ["it", "it-"],
            pt: ["pt", "pt-"],
            ru: ["ru", "ru-"],
            zh: ["zh", "zh-"],
            ja: ["ja", "ja-"],
            ko: ["ko", "ko-"],
            ar: ["ar", "ar-"],
            hi: ["hi", "hi-"],
          };

          const targetLangPrefixes = voiceLangMap[this.currentLanguage] || [
            "en",
            "en-",
          ];

          // Find voices that match the current language
          const matchingVoices = this.availableVoices.filter((voice) =>
            targetLangPrefixes.some((prefix) => voice.lang.startsWith(prefix))
          );

          // If we found matching voices, select the first one
          if (matchingVoices.length > 0) {
            this.selectedVoice = matchingVoices[0];
            localStorage.setItem("voicevision_voice", this.selectedVoice.name);
          }
        }

        async initializeAI() {
          try {
            this.updateStatus(
              "ü§ñ Loading AI models (TensorFlow.js, Tesseract, Gemma 3n)...",
              true
            );

            // Initialize Google Generative AI with Gemma 3n
            const API_KEY = "AIzaSyAucfVLeCDWbuxe0Osfka0U83Plqc0Bbog"; // Pre-configured API key

            this.genAI = new GoogleGenerativeAI(API_KEY);
            this.gemmaModel = this.genAI.getGenerativeModel({
              model: "gemini-1.5-flash", // Using latest available model
              generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 1024,
              },
            });
            console.log("‚úÖ Gemma 3n AI model loaded");

            // Load TensorFlow COCO-SSD model for object detection
            this.cocoModel = await cocoSsd.load();
            console.log("‚úÖ Object detection model loaded");

            // Initialize Tesseract worker for OCR
            this.tesseractWorker = await Tesseract.createWorker("eng");
            console.log("‚úÖ OCR engine loaded");

            // Initialize MediaPipe Hands for sign language detection
            if (this.signLanguageEnabled) {
              await this.initializeSignLanguage();
            }

            this.updateStatus("üéØ All AI models loaded successfully!");
          } catch (error) {
            console.error("AI initialization error:", error);
            this.updateStatus(
              "‚ö†Ô∏è Some AI models failed to load - continuing with available models"
            );
          }
        }

        initializeVoices() {
          // Load saved voice settings
          const savedSpeed = localStorage.getItem("voicevision_speed");
          if (savedSpeed) {
            this.voiceSettings.rate = parseFloat(savedSpeed);
          }

          const savedVolume = localStorage.getItem("voicevision_volume");
          if (savedVolume) {
            this.voiceSettings.volume = parseFloat(savedVolume);
          }

          // Load available voices when they become ready
          if ("speechSynthesis" in window) {
            const loadVoices = () => {
              this.availableVoices = speechSynthesis.getVoices();

              // Try to set a good default voice
              const savedVoice = localStorage.getItem("voicevision_voice");
              if (savedVoice && this.availableVoices.length > 0) {
                this.selectedVoice = this.availableVoices.find(
                  (v) => v.name === savedVoice
                );
              }

              // If no saved voice or not found, pick a good default
              if (!this.selectedVoice && this.availableVoices.length > 0) {
                // Prefer English voices
                const englishVoices = this.availableVoices.filter(
                  (v) => v.lang.startsWith("en-") && !v.name.includes("Google")
                );

                // Look for natural sounding voices
                const preferredVoices = englishVoices.filter(
                  (v) =>
                    v.name.includes("Natural") ||
                    v.name.includes("Premium") ||
                    v.name.includes("Enhanced") ||
                    v.name.includes("Neural") ||
                    v.name.includes("Samantha") ||
                    v.name.includes("Alex") ||
                    v.name.includes("Victoria")
                );

                this.selectedVoice =
                  preferredVoices[0] ||
                  englishVoices[0] ||
                  this.availableVoices[0];
              }

              console.log(
                `‚úÖ Voice system initialized with ${this.availableVoices.length} voices`
              );
              if (this.selectedVoice) {
                console.log(
                  `üé§ Selected voice: ${this.selectedVoice.name} (${this.selectedVoice.lang})`
                );
              }
            };

            // Load voices immediately if available
            loadVoices();

            // Also listen for voice changes (some browsers load voices asynchronously)
            speechSynthesis.onvoiceschanged = loadVoices;
          }
        }

        setupEventListeners() {
          // Button event listeners
          document
            .getElementById("start-btn")
            .addEventListener("click", () => this.startCamera());
          document
            .getElementById("describe-btn")
            .addEventListener("click", () => this.describeScene());
          document
            .getElementById("read-btn")
            .addEventListener("click", () => this.readText());
          document
            .getElementById("objects-btn")
            .addEventListener("click", () => this.identifyObjects());
          document
            .getElementById("stop-btn")
            .addEventListener("click", () => this.stopCamera());
          document
            .getElementById("stop-speech-btn")
            .addEventListener("click", () => this.stopSpeech());
          document
            .getElementById("voice-select-btn")
            .addEventListener("click", () => this.showVoiceSettings());
          document
            .getElementById("language-btn")
            .addEventListener("click", () => this.showLanguageSettings());
          document
            .getElementById("sign-language-btn")
            .addEventListener("click", () => this.toggleSignLanguage());
          document
            .getElementById("api-key-btn")
            .addEventListener("click", () => this.updateAPIKey());

          // Keyboard shortcuts
          document.addEventListener("keydown", (e) => {
            if (!this.isRunning) return;

            switch (e.key.toLowerCase()) {
              case "d":
                this.describeScene();
                break;
              case "r":
                this.readText();
                break;
              case "o":
                this.identifyObjects();
                break;
              case "q":
                this.stopCamera();
                break;
              case "s":
                this.stopSpeech();
                break;
              case " ":
                e.preventDefault();
                if (this.lastDescription) {
                  this.speak(this.lastDescription);
                }
                break;
            }
          });

          // Window resize handler to update canvas overlay
          window.addEventListener("resize", () => {
            if (this.isRunning && this.handLandmarksCanvas) {
              setTimeout(() => this.resizeCanvasOverlay(), 100);
            }
          });
        }

        async startCamera() {
          try {
            this.updateStatus(
              "üîÑ Starting camera and initializing AI systems...",
              true
            );

            // Request camera access
            this.stream = await navigator.mediaDevices.getUserMedia({
              video: {
                width: { ideal: 1280 },
                height: { ideal: 720 },
                facingMode: "environment",
              },
              audio: false,
            });

            this.video.srcObject = this.stream;

            await new Promise((resolve) => {
              this.video.onloadedmetadata = resolve;
            });

            // Wait for video to actually start playing to get accurate dimensions
            await new Promise((resolve) => {
              this.video.onplaying = resolve;
              this.video.play();
            });

            // Set up canvas overlay for hand landmarks to match displayed video size
            this.resizeCanvasOverlay();

            // Start hand tracking if sign language is enabled
            if (this.signLanguageEnabled && this.handsModel) {
              this.startHandTracking();
            }

            this.isRunning = true;
            this.updateStatus(
              "‚úÖ VoiceVision is active! Camera and AI systems ready for assistance."
            );
            this.enableControls(true);

            const signLanguageStatus = this.signLanguageEnabled
              ? " Sign language detection is active."
              : "";
            this.speak(
              `VoiceVision camera and AI systems are now active. I can describe scenes, read text, or identify objects. Use the buttons or keyboard shortcuts.${signLanguageStatus}`
            );
          } catch (error) {
            console.error("Startup error:", error);
            this.updateStatus(
              "‚ùå Failed to access camera. Please allow camera permissions and try again."
            );
            this.speak(
              "Camera access failed. Please check your permissions and try again."
            );
          }
        }

        startHandTracking() {
          if (!this.handsModel || !this.signLanguageEnabled) return;

          const sendFrame = async () => {
            if (this.isRunning && this.signLanguageEnabled) {
              await this.handsModel.send({ image: this.video });
              requestAnimationFrame(sendFrame);
            }
          };

          requestAnimationFrame(sendFrame);
        }

        stopHandTracking() {
          // Clear the canvas
          if (this.handLandmarksCtx) {
            this.handLandmarksCtx.clearRect(
              0,
              0,
              this.handLandmarksCanvas.width,
              this.handLandmarksCanvas.height
            );
          }
          this.gestureBuffer = [];
        }

        resizeCanvasOverlay() {
          // Get the actual displayed dimensions of the video element
          const videoRect = this.video.getBoundingClientRect();
          const videoComputedStyle = window.getComputedStyle(this.video);

          // Account for borders
          const borderWidth = parseInt(videoComputedStyle.borderWidth) || 3;

          // Set canvas dimensions to match the displayed video size
          this.handLandmarksCanvas.width = videoRect.width - borderWidth * 2;
          this.handLandmarksCanvas.height = videoRect.height - borderWidth * 2;

          // Set canvas CSS dimensions to match
          this.handLandmarksCanvas.style.width = `${
            videoRect.width - borderWidth * 2
          }px`;
          this.handLandmarksCanvas.style.height = `${
            videoRect.height - borderWidth * 2
          }px`;

          console.log(
            `Canvas resized to: ${this.handLandmarksCanvas.width}x${this.handLandmarksCanvas.height}`
          );
        }

        async describeScene() {
          if (!this.isRunning) return;

          try {
            this.updateStatus("üß† Analyzing scene with Gemma 3n AI...", true);

            // Capture current frame for analysis
            const canvas = this.captureFrame();
            const imageDataUrl = canvas.toDataURL("image/jpeg", 0.8);

            let description = "";

            if (this.gemmaModel) {
              // Use Gemma 3n for intelligent scene analysis
              description = await this.getGemmaSceneDescription(imageDataUrl);
            } else {
              // Fallback to basic analysis
              description = await this.getBasicSceneDescription(canvas);
            }

            this.lastDescription = description;
            this.updateStatus(`üîä Gemma 3n Analysis: "${description}"`);
            this.speak(description);
          } catch (error) {
            console.error("Scene analysis error:", error);
            this.updateStatus("‚ùå Error analyzing scene");
            this.speak(
              "Sorry, I encountered an error while analyzing the scene."
            );
          }
        }

        async getGemmaSceneDescription(imageDataUrl) {
          try {
            // Convert image to format Gemma can understand
            const imageData = {
              inlineData: {
                data: imageDataUrl.split(",")[1], // Remove data:image/jpeg;base64, prefix
                mimeType: "image/jpeg",
              },
            };

            const prompt = this.getSceneAnalysisPrompt();

            const result = await this.gemmaModel.generateContent([
              prompt,
              imageData,
            ]);
            const response = await result.response;
            const description = response.text();

            return (
              description ||
              "I can see a scene but I'm having trouble describing it clearly."
            );
          } catch (error) {
            console.error("Gemma analysis error:", error);
            // Fallback to basic analysis
            return await this.getBasicSceneDescription();
          }
        }

        async getBasicSceneDescription(canvas) {
          // Fallback analysis using TensorFlow.js + basic image processing
          let description = "I can see ";

          if (this.cocoModel) {
            const predictions = await this.cocoModel.detect(canvas);

            if (predictions.length > 0) {
              const objects = predictions
                .filter((p) => p.score > 0.5)
                .map(
                  (p) => `${p.class} (${Math.round(p.score * 100)}% confidence)`
                )
                .slice(0, 5);

              if (objects.length > 0) {
                description += objects.join(", ") + ". ";
                description += this.analyzeSceneComposition(predictions);
              } else {
                description +=
                  "some objects but they're not clearly identifiable. ";
              }
            } else {
              description += "a scene but no specific objects are detected. ";
            }
          } else {
            description += "a scene. Basic object detection is not available. ";
          }

          // Add lighting analysis
          if (canvas) {
            description += this.analyzeLightingAndEnvironment(canvas);
          }

          return description;
        }

        async readText() {
          if (!this.isRunning) return;

          try {
            this.updateStatus("üìñ Processing text with real OCR...", true);

            const canvas = this.captureFrame();

            if (this.tesseractWorker) {
              // Use real Tesseract OCR
              const {
                data: { text },
              } = await this.tesseractWorker.recognize(canvas);

              const cleanText = text.trim();

              if (cleanText && cleanText.length > 2) {
                const result = `I found text: "${cleanText}"`;
                this.updateStatus(`üìù Text Reading: "${result}"`);
                this.speak(result);
              } else {
                const result =
                  "No readable text detected in the current view. Try pointing the camera directly at text with good lighting.";
                this.updateStatus(`üìù Text Reading: "${result}"`);
                this.speak(result);
              }
            } else {
              const result =
                "OCR engine not available. Please try again after initialization.";
              this.updateStatus(`üìù Text Reading: "${result}"`);
              this.speak(result);
            }
          } catch (error) {
            console.error("Text reading error:", error);
            this.updateStatus("‚ùå Error reading text");
            this.speak("Sorry, I encountered an error while reading text.");
          }
        }

        async identifyObjects() {
          if (!this.isRunning) return;

          try {
            this.updateStatus("üéØ Analyzing objects with Gemma 3n AI...", true);

            const canvas = this.captureFrame();
            const imageDataUrl = canvas.toDataURL("image/jpeg", 0.8);

            let result = "";

            if (this.gemmaModel) {
              // Use Gemma 3n for intelligent object identification
              result = await this.getGemmaObjectAnalysis(imageDataUrl);
            } else {
              // Fallback to TensorFlow.js object detection
              result = await this.getBasicObjectDetection(canvas);
            }

            this.updateStatus(`üéØ Gemma 3n Object Analysis: ${result}`);
            this.speak(result);
          } catch (error) {
            console.error("Object detection error:", error);
            this.updateStatus("‚ùå Error identifying objects");
            this.speak(
              "Sorry, I encountered an error while identifying objects."
            );
          }
        }

        async getGemmaObjectAnalysis(imageDataUrl) {
          try {
            const imageData = {
              inlineData: {
                data: imageDataUrl.split(",")[1],
                mimeType: "image/jpeg",
              },
            };

            const prompt = this.getObjectIdentificationPrompt();

            const result = await this.gemmaModel.generateContent([
              prompt,
              imageData,
            ]);
            const response = await result.response;
            const description = response.text();

            return (
              description ||
              "I can see objects but I'm having trouble identifying them clearly."
            );
          } catch (error) {
            console.error("Gemma object analysis error:", error);
            return await this.getBasicObjectDetection();
          }
        }

        async getBasicObjectDetection(canvas) {
          if (this.cocoModel) {
            const predictions = await this.cocoModel.detect(canvas);

            if (predictions.length > 0) {
              const confidentObjects = predictions
                .filter((p) => p.score > 0.3)
                .sort((a, b) => b.score - a.score)
                .slice(0, 10);

              if (confidentObjects.length > 0) {
                const objectList = confidentObjects.map(
                  (p) => `${p.class} (${Math.round(p.score * 100)}% confidence)`
                );

                return `I can identify ${
                  confidentObjects.length
                } objects: ${objectList.join(", ")}`;
              } else {
                return "I can see objects but cannot identify them with high confidence. Try adjusting lighting or camera angle.";
              }
            } else {
              return "No objects detected. Try pointing the camera at different objects or improving lighting.";
            }
          } else {
            return "Object detection not available. Please wait for initialization.";
          }
        }

        getSceneAnalysisPrompt() {
          const prompts = this.languagePrompts[this.currentLanguage];
          return prompts
            ? prompts.sceneAnalysis
            : this.languagePrompts.en.sceneAnalysis;
        }

        getObjectIdentificationPrompt() {
          const prompts = this.languagePrompts[this.currentLanguage];
          return prompts
            ? prompts.objectIdentification
            : this.languagePrompts.en.objectIdentification;
        }

        async stopCamera() {
          // Immediately stop any ongoing speech
          if ("speechSynthesis" in window) {
            speechSynthesis.cancel();
          }

          if (this.stream) {
            this.stream.getTracks().forEach((track) => track.stop());
            this.stream = null;
          }

          // Stop hand tracking
          this.stopHandTracking();

          // Clean up AI resources
          if (this.tesseractWorker) {
            await this.tesseractWorker.terminate();
            this.tesseractWorker = null;
          }

          this.isRunning = false;
          this.updateStatus(
            'üì¢ VoiceVision stopped. Click "Start Camera & AI" to begin again.'
          );
          this.enableControls(false);

          // Use a short delay before speaking the stop message to ensure previous speech is cancelled
          setTimeout(() => {
            this.speak("VoiceVision has been stopped.");
          }, 100);
        }

        captureFrame() {
          const canvas = document.createElement("canvas");
          canvas.width = this.video.videoWidth || 640;
          canvas.height = this.video.videoHeight || 480;

          const context = canvas.getContext("2d");
          context.drawImage(this.video, 0, 0, canvas.width, canvas.height);

          return canvas; // Return canvas element for AI processing
        }

        analyzeSceneComposition(predictions) {
          if (!predictions || predictions.length === 0) {
            return "The scene appears to have low visual complexity.";
          }

          const objectCount = predictions.filter((p) => p.score > 0.5).length;

          if (objectCount === 0) {
            return "The scene has minimal identifiable objects.";
          } else if (objectCount <= 2) {
            return "This is a simple scene with a few main objects.";
          } else if (objectCount <= 5) {
            return "This is a moderately complex scene with several objects.";
          } else {
            return "This is a complex scene with many objects and details.";
          }
        }

        analyzeLightingAndEnvironment(canvas) {
          const context = canvas.getContext("2d");
          const imageData = context.getImageData(
            0,
            0,
            canvas.width,
            canvas.height
          );
          const data = imageData.data;

          // Calculate average brightness
          let totalBrightness = 0;
          for (let i = 0; i < data.length; i += 4) {
            const r = data[i];
            const g = data[i + 1];
            const b = data[i + 2];
            totalBrightness += (r + g + b) / 3;
          }

          const avgBrightness = totalBrightness / (data.length / 4);

          let lightingDescription = "";
          if (avgBrightness < 60) {
            lightingDescription = "The lighting is quite dim.";
          } else if (avgBrightness < 120) {
            lightingDescription = "The lighting is moderate.";
          } else if (avgBrightness < 180) {
            lightingDescription = "The lighting is good.";
          } else {
            lightingDescription = "The lighting is very bright.";
          }

          return lightingDescription;
        }

        updateStatus(message, showLoading = false) {
          const loadingSpinner = showLoading
            ? '<span class="loading"></span>'
            : "";
          this.statusText.innerHTML = loadingSpinner + message;
          console.log(message);
        }

        enableControls(enabled) {
          const buttons = [
            "describe-btn",
            "read-btn",
            "objects-btn",
            "stop-btn",
          ];
          buttons.forEach((id) => {
            const btn = document.getElementById(id);
            if (btn) btn.disabled = !enabled;
          });

          const startBtn = document.getElementById("start-btn");
          if (startBtn) startBtn.disabled = enabled;
        }

        speak(text) {
          if ("speechSynthesis" in window) {
            // Cancel any ongoing speech first
            speechSynthesis.cancel();

            // Small delay to ensure cancellation is processed
            setTimeout(() => {
              const utterance = new SpeechSynthesisUtterance(text);

              // Apply voice settings
              utterance.rate = this.voiceSettings.rate;
              utterance.pitch = this.voiceSettings.pitch;
              utterance.volume = this.voiceSettings.volume;

              // Use selected voice if available
              if (this.selectedVoice) {
                utterance.voice = this.selectedVoice;
              }

              speechSynthesis.speak(utterance);
            }, 50);
          } else {
            console.log("üîä Would speak:", text);
          }
        }

        stopSpeech() {
          if ("speechSynthesis" in window) {
            speechSynthesis.cancel();
          }
        }

        delay(ms) {
          return new Promise((resolve) => setTimeout(resolve, ms));
        }

        showVoiceSettings() {
          if (!this.availableVoices || this.availableVoices.length === 0) {
            alert(
              "üé§ Voice system not ready yet. Please try again in a moment."
            );
            return;
          }

          // Create voice options list
          let voiceOptions = "üé§ Available Voices:\n\n";
          this.availableVoices.forEach((voice, index) => {
            const isSelected =
              this.selectedVoice && voice.name === this.selectedVoice.name
                ? " ‚úì"
                : "";
            voiceOptions += `${index + 1}. ${voice.name} (${
              voice.lang
            })${isSelected}\n`;
          });

          voiceOptions += `\nCurrent Settings:
üìà Speed: ${this.voiceSettings.rate}
üéµ Pitch: ${this.voiceSettings.pitch}
üîä Volume: ${this.voiceSettings.volume}

Enter the number of the voice you want to use:`;

          const selection = prompt(voiceOptions);

          if (selection) {
            const voiceIndex = parseInt(selection) - 1;
            if (voiceIndex >= 0 && voiceIndex < this.availableVoices.length) {
              this.selectedVoice = this.availableVoices[voiceIndex];
              localStorage.setItem(
                "voicevision_voice",
                this.selectedVoice.name
              );

              // Test the new voice
              this.speak(
                `Voice changed to ${this.selectedVoice.name}. This is how I sound now.`
              );

              // Ask for speed adjustment
              this.adjustVoiceSettings();
            } else {
              alert("‚ùå Invalid selection. Please try again.");
            }
          }
        }

        showLanguageSettings() {
          // Create language options list
          let languageOptions = "üåç Available Languages:\n\n";
          Object.entries(this.languages).forEach(([code, name], index) => {
            const isSelected = this.currentLanguage === code ? " ‚úì" : "";
            languageOptions += `${index + 1}. ${name}${isSelected}\n`;
          });

          languageOptions += `\nCurrent Language: ${
            this.languages[this.currentLanguage]
          }\n\nEnter the number of the language you want to use:`;

          const selection = prompt(languageOptions);

          if (selection) {
            const languageIndex = parseInt(selection) - 1;
            const languageCodes = Object.keys(this.languages);

            if (languageIndex >= 0 && languageIndex < languageCodes.length) {
              const newLanguage = languageCodes[languageIndex];
              this.currentLanguage = newLanguage;
              localStorage.setItem("voicevision_language", newLanguage);

              // Update Tesseract language
              this.updateTesseractLanguage();

              // Filter voices by new language
              this.filterVoicesByLanguage();

              // Update UI
              this.initializeLanguage();

              // Test the new language
              const testMessages = {
                en: "Language changed to English. All AI responses will now be in English.",
                es: "Idioma cambiado a espa√±ol. Todas las respuestas de IA ahora estar√°n en espa√±ol.",
                fr: "Langue chang√©e en fran√ßais. Toutes les r√©ponses IA seront maintenant en fran√ßais.",
                de: "Sprache auf Deutsch ge√§ndert. Alle KI-Antworten werden jetzt auf Deutsch sein.",
                it: "Lingua cambiata in italiano. Tutte le risposte AI saranno ora in italiano.",
                pt: "Idioma alterado para portugu√™s. Todas as respostas de IA agora estar√£o em portugu√™s.",
                ru: "–Ø–∑—ã–∫ –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ —Ä—É—Å—Å–∫–∏–π. –í—Å–µ –æ—Ç–≤–µ—Ç—ã –ò–ò —Ç–µ–ø–µ—Ä—å –±—É–¥—É—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.",
                zh: "ËØ≠Ë®ÄÂ∑≤Êõ¥Êîπ‰∏∫‰∏≠Êñá„ÄÇÊâÄÊúâAIÂõûÂ§çÁé∞Âú®ÈÉΩÂ∞Ü‰ΩøÁî®‰∏≠Êñá„ÄÇ",
                ja: "Ë®ÄË™û„ÇíÊó•Êú¨Ë™û„Å´Â§âÊõ¥„Åó„Åæ„Åó„Åü„ÄÇ„Åô„Åπ„Å¶„ÅÆAIÂøúÁ≠î„ÅØÊó•Êú¨Ë™û„Å´„Å™„Çä„Åæ„Åô„ÄÇ",
                ko: "Ïñ∏Ïñ¥Í∞Ä ÌïúÍµ≠Ïñ¥Î°ú Î≥ÄÍ≤ΩÎêòÏóàÏäµÎãàÎã§. Î™®Îì† AI ÏùëÎãµÏù¥ Ïù¥Ï†ú ÌïúÍµ≠Ïñ¥Î°ú Ï†úÍ≥µÎê©ÎãàÎã§.",
                ar: "ÿ™ŸÖ ÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸÑÿ∫ÿ© ÿ•ŸÑŸâ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©. ÿ≥ÿ™ŸÉŸàŸÜ ÿ¨ŸÖŸäÿπ ÿßÿ≥ÿ™ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸÑÿ¢ŸÜ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©.",
                hi: "‡§≠‡§æ‡§∑‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à‡•§ ‡§Ö‡§¨ ‡§∏‡§≠‡•Ä AI ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡§è‡§Ç ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§Ç‡§ó‡•Ä‡•§",
              };

              this.speak(testMessages[newLanguage] || testMessages.en);
            } else {
              alert("‚ùå Invalid selection. Please try again.");
            }
          }
        }

        adjustVoiceSettings() {
          const speedInput = prompt(
            "üé§ Voice Speed Settings:\n\n" +
              "Current speed: " +
              this.voiceSettings.rate +
              "\n" +
              "Enter new speed (0.1 to 2.0, where 1.0 is normal):\n" +
              "‚Ä¢ 0.5 = Very slow\n" +
              "‚Ä¢ 0.8 = Slow\n" +
              "‚Ä¢ 1.0 = Normal\n" +
              "‚Ä¢ 1.2 = Fast\n" +
              "‚Ä¢ 1.5 = Very fast\n\n" +
              "(Press Cancel to keep current settings)"
          );

          if (speedInput !== null) {
            const newSpeed = parseFloat(speedInput);
            if (newSpeed >= 0.1 && newSpeed <= 2.0) {
              this.voiceSettings.rate = newSpeed;
              localStorage.setItem("voicevision_speed", newSpeed.toString());
              this.speak(
                `Speed adjusted to ${newSpeed}. This is the new speaking speed.`
              );
            } else if (speedInput.trim() !== "") {
              alert("‚ùå Please enter a speed between 0.1 and 2.0");
            }
          }
        }

        async initializeSignLanguage() {
          try {
            this.updateStatus(
              "ü§ü Initializing sign language detection...",
              true
            );

            // Initialize MediaPipe Hands
            this.handsModel = new Hands({
              locateFile: (file) => {
                return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
              },
            });

            this.handsModel.setOptions({
              maxNumHands: 2,
              modelComplexity: 1,
              minDetectionConfidence: 0.5,
              minTrackingConfidence: 0.5,
            });

            this.handsModel.onResults(this.onHandsResults.bind(this));

            console.log("‚úÖ Sign language detection initialized");
          } catch (error) {
            console.error("Sign language initialization error:", error);
            this.updateStatus(
              "‚ö†Ô∏è Sign language detection failed to initialize"
            );
          }
        }

        onHandsResults(results) {
          if (!this.signLanguageEnabled || !this.isRunning) return;

          // Clear canvas
          this.handLandmarksCtx.clearRect(
            0,
            0,
            this.handLandmarksCanvas.width,
            this.handLandmarksCanvas.height
          );

          if (results.multiHandLandmarks) {
            // Draw hand landmarks
            for (const landmarks of results.multiHandLandmarks) {
              this.drawHandLandmarks(landmarks);
            }

            // Detect gestures
            this.detectGestures(results.multiHandLandmarks);
          }
        }

        drawHandLandmarks(landmarks) {
          const ctx = this.handLandmarksCtx;
          ctx.fillStyle = "#FF0000";
          ctx.strokeStyle = "#00FF00";
          ctx.lineWidth = 2;

          // Draw connections between landmarks
          const connections = [
            [0, 1],
            [1, 2],
            [2, 3],
            [3, 4], // thumb
            [0, 5],
            [5, 6],
            [6, 7],
            [7, 8], // index finger
            [5, 9],
            [9, 10],
            [10, 11],
            [11, 12], // middle finger
            [9, 13],
            [13, 14],
            [14, 15],
            [15, 16], // ring finger
            [13, 17],
            [17, 18],
            [18, 19],
            [19, 20], // pinky
            [0, 17], // palm
          ];

          // Draw connections
          ctx.beginPath();
          for (const [start, end] of connections) {
            const startPoint = landmarks[start];
            const endPoint = landmarks[end];
            ctx.moveTo(
              startPoint.x * this.handLandmarksCanvas.width,
              startPoint.y * this.handLandmarksCanvas.height
            );
            ctx.lineTo(
              endPoint.x * this.handLandmarksCanvas.width,
              endPoint.y * this.handLandmarksCanvas.height
            );
          }
          ctx.stroke();

          // Draw landmarks
          for (const landmark of landmarks) {
            ctx.beginPath();
            ctx.arc(
              landmark.x * this.handLandmarksCanvas.width,
              landmark.y * this.handLandmarksCanvas.height,
              3,
              0,
              2 * Math.PI
            );
            ctx.fill();
          }
        }

        detectGestures(multiHandLandmarks) {
          if (multiHandLandmarks.length === 0) return;

          const now = Date.now();
          if (now - this.lastGestureTime < 2000) return; // Throttle gesture detection

          for (const landmarks of multiHandLandmarks) {
            const gesture = this.classifyGesture(landmarks);
            if (gesture) {
              // Always allow pause/resume gesture
              if (gesture === "pause" || !this.systemPaused) {
                this.gestureBuffer.push(gesture);

                // If we have enough consistent gestures, execute the command
                if (this.gestureBuffer.length >= this.gestureThreshold) {
                  const mostCommonGesture = this.getMostCommonGesture();
                  if (mostCommonGesture) {
                    this.executeGestureCommand(mostCommonGesture);
                    this.lastGestureTime = now;
                    this.gestureBuffer = [];
                  }
                }

                // Keep buffer size manageable
                if (this.gestureBuffer.length > this.gestureThreshold * 2) {
                  this.gestureBuffer = this.gestureBuffer.slice(
                    -this.gestureThreshold
                  );
                }
              }
            }
          }
        }

        classifyGesture(landmarks) {
          // Simple gesture classification based on finger positions
          const fingerTips = [4, 8, 12, 16, 20]; // thumb, index, middle, ring, pinky tips
          const fingerMcps = [3, 6, 10, 14, 18]; // finger base joints

          // Check which fingers are extended
          const fingersUp = [];

          // Thumb (different logic due to orientation)
          if (landmarks[4].x > landmarks[3].x) {
            fingersUp.push(1);
          } else {
            fingersUp.push(0);
          }

          // Other fingers
          for (let i = 1; i < 5; i++) {
            if (landmarks[fingerTips[i]].y < landmarks[fingerMcps[i]].y) {
              fingersUp.push(1);
            } else {
              fingersUp.push(0);
            }
          }

          // Classify gestures based on finger patterns
          const pattern = fingersUp.join("");

          switch (pattern) {
            case "01000": // Index finger only - Describe Scene
              return "describe";
            case "01100": // Index and middle finger - Read Text
              return "read";
            case "01110": // Index, middle, ring finger - Navigation Help
              return "navigate";
            case "00001": // Pinky only - Identify Objects
              return "objects";
            case "00010": // Ring finger only - Voice Settings
              return "voice";
            case "00100": // Middle finger only - Language Settings
              return "language";
            case "11000": // Thumb and index - Zoom In/Brightness
              return "brightness";
            case "10100": // Thumb and middle - Emergency Help
              return "emergency";
            case "11100": // Thumb, index, middle - Take Photo
              return "photo";
            case "11110": // All except pinky - Volume Control
              return "volume";
            case "11111": // All fingers - Stop
              return "stop";
            case "10000": // Thumb only - Repeat last
              return "repeat";
            case "00000": // Closed fist - Pause/Resume
              return "pause";
            case "01010": // Index and ring finger - Help/Tutorial
              return "help";
            case "10010": // Thumb and ring finger - Save Description
              return "save";
            case "10001": // Thumb and pinky - Toggle Speech Speed
              return "speed";
            default:
              return null;
          }
        }

        getMostCommonGesture() {
          if (this.gestureBuffer.length === 0) return null;

          const counts = {};
          for (const gesture of this.gestureBuffer) {
            counts[gesture] = (counts[gesture] || 0) + 1;
          }

          let maxCount = 0;
          let mostCommon = null;
          for (const [gesture, count] of Object.entries(counts)) {
            if (count > maxCount) {
              maxCount = count;
              mostCommon = gesture;
            }
          }

          // Require at least 60% consistency
          return maxCount >= this.gestureThreshold * 0.6 ? mostCommon : null;
        }

        executeGestureCommand(gesture) {
          const gestureMessages = {
            describe: "Hand gesture detected: Describing scene",
            read: "Hand gesture detected: Reading text",
            navigate: "Hand gesture detected: Navigation assistance",
            objects: "Hand gesture detected: Identifying objects",
            voice: "Hand gesture detected: Opening voice settings",
            language: "Hand gesture detected: Opening language settings",
            brightness: "Hand gesture detected: Adjusting brightness",
            emergency: "Hand gesture detected: Emergency help mode",
            photo: "Hand gesture detected: Taking photo for analysis",
            volume: "Hand gesture detected: Adjusting volume",
            stop: "Hand gesture detected: Stopping camera",
            repeat: "Hand gesture detected: Repeating last description",
            pause: "Hand gesture detected: Pausing system",
            help: "Hand gesture detected: Opening help tutorial",
            save: "Hand gesture detected: Saving current description",
            speed: "Hand gesture detected: Adjusting speech speed",
          };

          const message = gestureMessages[gesture];
          if (message) {
            this.updateStatus(`ü§ü ${message}`);
            this.speak(message);

            // Execute the corresponding action
            setTimeout(() => {
              switch (gesture) {
                case "describe":
                  this.describeScene();
                  break;
                case "read":
                  this.readText();
                  break;
                case "navigate":
                  this.provideNavigationHelp();
                  break;
                case "objects":
                  this.identifyObjects();
                  break;
                case "voice":
                  this.showVoiceSettings();
                  break;
                case "language":
                  this.showLanguageSettings();
                  break;
                case "brightness":
                  this.adjustBrightness();
                  break;
                case "emergency":
                  this.activateEmergencyMode();
                  break;
                case "photo":
                  this.takePhotoForAnalysis();
                  break;
                case "volume":
                  this.adjustVolume();
                  break;
                case "stop":
                  this.stopCamera();
                  break;
                case "repeat":
                  if (this.lastDescription) {
                    this.speak(this.lastDescription);
                  } else {
                    this.speak("No previous description to repeat");
                  }
                  break;
                case "pause":
                  this.pauseSystem();
                  break;
                case "help":
                  this.showHelpTutorial();
                  break;
                case "save":
                  this.saveCurrentDescription();
                  break;
                case "speed":
                  this.toggleSpeechSpeed();
                  break;
              }
            }, 1000); // Small delay to allow gesture message to be heard
          }
        }

        provideNavigationHelp() {
          if (!this.isRunning) return;

          try {
            this.updateStatus(
              "üß≠ Analyzing environment for navigation...",
              true
            );
            const canvas = this.captureFrame();

            // Simple navigation analysis based on scene complexity and objects
            const lightingInfo = this.analyzeLightingAndEnvironment(canvas);

            let navigationAdvice = `Navigation analysis: ${lightingInfo} `;

            if (this.cocoModel) {
              this.cocoModel.detect(canvas).then((predictions) => {
                const obstacles = predictions.filter(
                  (p) =>
                    p.score > 0.5 &&
                    [
                      "person",
                      "chair",
                      "table",
                      "car",
                      "bicycle",
                      "motorcycle",
                    ].includes(p.class)
                );

                if (obstacles.length > 0) {
                  navigationAdvice += `I detect ${
                    obstacles.length
                  } potential obstacles: ${obstacles
                    .map((o) => o.class)
                    .join(", ")}. Please navigate carefully.`;
                } else {
                  navigationAdvice += "Path appears clear of major obstacles.";
                }

                this.updateStatus(`üß≠ ${navigationAdvice}`);
                this.speak(navigationAdvice);
              });
            } else {
              navigationAdvice +=
                "Navigation system ready but object detection not available.";
              this.updateStatus(`üß≠ ${navigationAdvice}`);
              this.speak(navigationAdvice);
            }
          } catch (error) {
            console.error("Navigation help error:", error);
            this.speak("Navigation assistance temporarily unavailable.");
          }
        }

        adjustBrightness() {
          if (!this.isRunning) return;

          const canvas = this.captureFrame();
          const lightingInfo = this.analyzeLightingAndEnvironment(canvas);

          this.speak(
            `Current lighting assessment: ${lightingInfo} You may want to adjust your device screen brightness or move to better lighting.`
          );
        }

        activateEmergencyMode() {
          this.speak(
            "Emergency mode activated. VoiceVision will provide continuous scene monitoring. To deactivate, use the stop gesture or say stop."
          );

          if (this.isRunning) {
            // Start continuous monitoring
            this.emergencyMode = true;
            this.startEmergencyMonitoring();
          }
        }

        startEmergencyMonitoring() {
          if (!this.emergencyMode || !this.isRunning) return;

          // Describe scene every 10 seconds in emergency mode
          this.describeScene();

          setTimeout(() => {
            this.startEmergencyMonitoring();
          }, 10000);
        }

        takePhotoForAnalysis() {
          if (!this.isRunning) return;

          try {
            const canvas = this.captureFrame();
            const dataUrl = canvas.toDataURL("image/jpeg", 0.8);

            // Store the photo data for detailed analysis
            this.savedPhoto = dataUrl;
            this.speak(
              "Photo captured for detailed analysis. Analyzing now..."
            );

            // Perform comprehensive analysis
            setTimeout(() => {
              this.describeScene();
              setTimeout(() => this.identifyObjects(), 2000);
              setTimeout(() => this.readText(), 4000);
            }, 1000);
          } catch (error) {
            console.error("Photo capture error:", error);
            this.speak("Unable to capture photo for analysis.");
          }
        }

        adjustVolume() {
          // Cycle through volume levels
          const volumeLevels = [0.3, 0.5, 0.8, 1.0];
          const currentIndex = volumeLevels.indexOf(this.voiceSettings.volume);
          const nextIndex = (currentIndex + 1) % volumeLevels.length;

          this.voiceSettings.volume = volumeLevels[nextIndex];
          localStorage.setItem(
            "voicevision_volume",
            this.voiceSettings.volume.toString()
          );

          this.speak(
            `Volume adjusted to ${Math.round(
              this.voiceSettings.volume * 100
            )} percent.`
          );
        }

        pauseSystem() {
          if (this.isRunning) {
            this.systemPaused = !this.systemPaused;

            if (this.systemPaused) {
              this.speak(
                "VoiceVision paused. Use closed fist gesture again to resume."
              );
              this.updateStatus(
                "‚è∏Ô∏è System paused - use closed fist gesture to resume"
              );
            } else {
              this.speak("VoiceVision resumed and ready for commands.");
              this.updateStatus("‚ñ∂Ô∏è System resumed and active");
            }
          }
        }

        showHelpTutorial() {
          const helpText = `VoiceVision sign language gestures: 
          Index finger for scene description, 
          Two fingers for reading text, 
          Three fingers for navigation help,
          Ring finger for voice settings,
          Middle finger for language settings,
          Thumb and index for brightness,
          Thumb and middle for emergency mode,
          Thumb index and middle for photo capture,
          All fingers except pinky for volume,
          All fingers to stop,
          Thumb only to repeat,
          Closed fist to pause,
          Index and ring for this help,
          Thumb and ring to save description,
          Thumb and pinky for speech speed.`;

          this.speak(helpText);
          this.updateStatus("üÜò Help tutorial playing...");
        }

        saveCurrentDescription() {
          if (this.lastDescription) {
            // Save to localStorage with timestamp
            const timestamp = new Date().toLocaleString();
            const savedDescriptions = JSON.parse(
              localStorage.getItem("voicevision_saved") || "[]"
            );

            savedDescriptions.push({
              description: this.lastDescription,
              timestamp: timestamp,
            });

            // Keep only last 10 descriptions
            if (savedDescriptions.length > 10) {
              savedDescriptions.shift();
            }

            localStorage.setItem(
              "voicevision_saved",
              JSON.stringify(savedDescriptions)
            );
            this.speak("Current description saved successfully.");
          } else {
            this.speak("No description available to save.");
          }
        }

        toggleSpeechSpeed() {
          // Cycle through speed levels
          const speedLevels = [0.5, 0.7, 0.9, 1.0, 1.2, 1.5];
          const currentIndex = speedLevels.indexOf(this.voiceSettings.rate);
          const nextIndex = (currentIndex + 1) % speedLevels.length;

          this.voiceSettings.rate = speedLevels[nextIndex];
          localStorage.setItem(
            "voicevision_speed",
            this.voiceSettings.rate.toString()
          );

          this.speak(
            `Speech speed adjusted to ${this.voiceSettings.rate} times normal speed.`
          );
        }

        toggleSignLanguage() {
          this.signLanguageEnabled = !this.signLanguageEnabled;
          localStorage.setItem(
            "voicevision_sign_language",
            this.signLanguageEnabled.toString()
          );

          const button = document.getElementById("sign-language-btn");
          if (button) {
            button.textContent = `ü§ü Sign Language: ${
              this.signLanguageEnabled ? "ON" : "OFF"
            }`;
          }

          if (this.signLanguageEnabled) {
            this.initializeSignLanguage();
            this.speak(
              "Advanced sign language detection enabled. Available gestures: index finger for scene description, two fingers for reading text, three fingers for navigation, ring finger for voice settings, middle finger for language, thumb combinations for brightness and emergency mode, closed fist to pause, and many more. Use index and ring fingers together for complete gesture help."
            );
          } else {
            this.speak("Sign language detection disabled.");
            // Clear canvas
            if (this.handLandmarksCtx) {
              this.handLandmarksCtx.clearRect(
                0,
                0,
                this.handLandmarksCanvas.width,
                this.handLandmarksCanvas.height
              );
            }
          }
        }

        updateAPIKey() {
          alert(
            "üîë API Key Information:\n\n" +
              "The Gemma 3n API key is pre-configured in this application.\n" +
              "No additional setup is required - Gemma 3n features are ready to use!\n\n" +
              "All AI models will initialize automatically when you start the camera."
          );
        }
      }

      // Initialize the interface when DOM is loaded
      document.addEventListener("DOMContentLoaded", () => {
        new VoiceVisionInterface();
      });
    </script>
  </body>
</html>
